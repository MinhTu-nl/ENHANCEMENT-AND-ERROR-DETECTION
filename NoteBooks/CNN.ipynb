{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"10G4RmuRo-maj5zXsY3QC5Z1dLWsTfVzy","authorship_tag":"ABX9TyMa5ZwpGiVtsYAtoUlPKO72"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"57U-XkxfBPSs","executionInfo":{"status":"ok","timestamp":1747197357640,"user_tz":-420,"elapsed":5828,"user":{"displayName":"Tú Nguyễn","userId":"18020195782305815633"}}},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","class LoLDataset(Dataset):\n","    def __init__(self, low_light_dir, high_light_dir, transform=None, target_size=(256, 256)):\n","        self.low_light_dir = low_light_dir\n","        self.high_light_dir = high_light_dir\n","        self.transform = transform\n","        self.target_size = target_size\n","        self.low_light_images = sorted(os.listdir(low_light_dir))\n","        self.high_light_images = sorted(os.listdir(high_light_dir))\n","\n","    def __len__(self):\n","        return len(self.low_light_images)\n","\n","    def __getitem__(self, idx):\n","        low_img_path = os.path.join(self.low_light_dir, self.low_light_images[idx])\n","        high_img_path = os.path.join(self.high_light_dir, self.high_light_images[idx])\n","\n","        low_img = cv2.imread(low_img_path)\n","        high_img = cv2.imread(high_img_path)\n","\n","        if low_img is None or high_img is None:\n","            raise ValueError(f\"Failed to load image at {low_img_path} or {high_img_path}\")\n","\n","        low_img = cv2.cvtColor(low_img, cv2.COLOR_BGR2RGB)\n","        high_img = cv2.cvtColor(high_img, cv2.COLOR_BGR2RGB)\n","\n","        # Resize images to target size\n","        low_img = cv2.resize(low_img, self.target_size, interpolation=cv2.INTER_AREA)\n","        high_img = cv2.resize(high_img, self.target_size, interpolation=cv2.INTER_AREA)\n","\n","        if self.transform:\n","            low_img = self.transform(low_img)\n","            high_img = self.transform(high_img)\n","\n","        return low_img, high_img\n","\n","def get_data_loaders(low_light_dir, high_light_dir, batch_size=16, target_size=(256, 256)):\n","    transform = lambda x: torch.from_numpy(x.transpose(2, 0, 1)).float() / 255.0\n","    dataset = LoLDataset(low_light_dir, high_light_dir, transform, target_size)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","def save_image(tensor, path):\n","    img = tensor.detach().cpu().numpy().transpose(1, 2, 0) * 255.0  # Thêm .detach()\n","    img = img.astype(np.uint8)\n","    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(path, img)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision.models import vgg16\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        # Encoder\n","        self.enc1 = nn.Sequential(nn.Conv2d(3, 64, 4, stride=2, padding=1), nn.ReLU())\n","        self.enc2 = nn.Sequential(nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.ReLU())\n","        self.enc3 = nn.Sequential(nn.Conv2d(128, 256, 4, stride=2, padding=1), nn.ReLU())\n","        self.enc4 = nn.Sequential(nn.Conv2d(256, 512, 4, stride=2, padding=1), nn.ReLU())\n","\n","        # Decoder with skip connections\n","        self.dec4 = nn.Sequential(nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), nn.ReLU())\n","        self.dec3 = nn.Sequential(nn.ConvTranspose2d(512, 128, 4, stride=2, padding=1), nn.ReLU())\n","        self.dec2 = nn.Sequential(nn.ConvTranspose2d(256, 64, 4, stride=2, padding=1), nn.ReLU())\n","        self.dec1 = nn.Sequential(nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1), nn.Tanh())\n","\n","    def forward(self, x):\n","        e1 = self.enc1(x)\n","        e2 = self.enc2(e1)\n","        e3 = self.enc3(e2)\n","        e4 = self.enc4(e3)\n","\n","        d4 = self.dec4(e4)\n","        d4 = torch.cat([d4, e3], dim=1)\n","        d3 = self.dec3(d4)\n","        d3 = torch.cat([d3, e2], dim=1)\n","        d2 = self.dec2(d3)\n","        d2 = torch.cat([d2, e1], dim=1)\n","        d1 = self.dec1(d2)\n","        return d1\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 1, 4, stride=1, padding=0),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class PerceptualLoss(nn.Module):\n","    def __init__(self, device):\n","        super(PerceptualLoss, self).__init__()\n","        vgg = vgg16(pretrained=True).features[:16].eval().to(device)\n","        for param in vgg.parameters():\n","            param.requires_grad = False\n","        self.vgg = vgg\n","        self.loss = nn.MSELoss()\n","\n","    def forward(self, fake, real):\n","        fake_features = self.vgg(fake)\n","        real_features = self.vgg(real)\n","        return self.loss(fake_features, real_features)\n","\n","def initialize_weights(model):\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        elif isinstance(m, nn.BatchNorm2d):\n","            nn.init.normal_(m.weight.data, 1.0, 0.02)\n","            nn.init.constant_(m.bias.data, 0)"],"metadata":{"id":"YW55l5XhFSMT","executionInfo":{"status":"ok","timestamp":1747197464016,"user_tz":-420,"elapsed":5718,"user":{"displayName":"Tú Nguyễn","userId":"18020195782305815633"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import os"],"metadata":{"id":"P3SbPMpmFfML","executionInfo":{"status":"ok","timestamp":1747197496957,"user_tz":-420,"elapsed":2,"user":{"displayName":"Tú Nguyễn","userId":"18020195782305815633"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def train_gan(epochs=100, batch_size=16, lr=0.00005, beta1=0.5, start_epoch=0, checkpoint_path=None):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Initialize models\n","    generator = Generator().to(device)\n","    discriminator = Discriminator().to(device)\n","    perceptual_loss = PerceptualLoss(device)\n","\n","    initialize_weights(generator)\n","    initialize_weights(discriminator)\n","\n","    # Load checkpoint if provided\n","    if checkpoint_path and os.path.exists(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","        generator.load_state_dict(checkpoint)\n","        print(f\"Loaded checkpoint from {checkpoint_path}\")\n","    else:\n","        print(\"No checkpoint loaded, starting from scratch or with initialized weights.\")\n","\n","    # Loss functions\n","    adversarial_loss = nn.BCELoss()\n","    pixel_loss = nn.L1Loss()\n","\n","    # Optimizers\n","    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n","    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n","\n","    # Data loaders/content/drive/MyDrive/\n","    low_light_dir = \"/content/drive/MyDrive/data/processed/our485/low\"\n","    high_light_dir = \"/content/drive/MyDrive/data/processed/our485/high\"\n","    train_loader = get_data_loaders(low_light_dir, high_light_dir, batch_size)\n","\n","    # Validation loader\n","    eval_loader = get_data_loaders(\"/content/drive/MyDrive/data/processed/eval15/low\", \"/content/drive/MyDrive/data/processed/eval15/high\", batch_size)\n","\n","\n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        generator.train()\n","        for i, (low_light, high_light) in enumerate(train_loader):\n","            low_light, high_light = low_light.to(device), high_light.to(device)\n","\n","            # Calculate the output size of the Discriminator\n","            with torch.no_grad():\n","                sample_output = discriminator(low_light)\n","                output_size = sample_output.shape[2:]  # e.g., (29, 29)\n","\n","            # Adjust labels to match the Discriminator's output size\n","            real_labels = torch.ones(low_light.size(0), 1, *output_size).to(device)\n","            fake_labels = torch.zeros(low_light.size(0), 1, *output_size).to(device)\n","\n","            # Train Discriminator\n","            d_optimizer.zero_grad()\n","\n","            real_output = discriminator(high_light)\n","            d_real_loss = adversarial_loss(real_output, real_labels)\n","\n","            fake_images = generator(low_light)\n","            fake_output = discriminator(fake_images.detach())\n","            d_fake_loss = adversarial_loss(fake_output, fake_labels)\n","\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","            d_loss.backward()\n","            d_optimizer.step()\n","\n","            # Train Generator\n","            g_optimizer.zero_grad()\n","            fake_output = discriminator(fake_images)\n","            g_adv_loss = adversarial_loss(fake_output, real_labels)\n","            g_pixel_loss = pixel_loss(fake_images, high_light)\n","            g_perceptual_loss = perceptual_loss(fake_images, high_light)\n","            g_loss = g_adv_loss + 10 * g_pixel_loss + 5 * g_perceptual_loss\n","            g_loss.backward()\n","            g_optimizer.step()\n","\n","            if i % 10 == 0:\n","                print(f\"Epoch [{epoch}/{start_epoch + epochs - 1}] Batch [{i}/{len(train_loader)}] \"\n","                    f\"D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n","\n","        # Evaluate on validation set\n","        if epoch % 10 == 0:\n","            generator.eval()\n","            with torch.no_grad():\n","                for low_light, high_light in eval_loader:\n","                    low_light, high_light = low_light.to(device), high_light.to(device)\n","                    fake_images = generator(low_light)\n","                    save_image(fake_images[0], f\"/content/drive/MyDrive/UPDATE/eval_epoch_{epoch}.png\")\n","                    break\n","            save_image(fake_images[0], f\"/content/drive/MyDrive/UPDATE/epoch_{epoch}.png\")\n","            torch.save(generator.state_dict(), f\"/content/drive/MyDrive/UPDATE/generator_epoch_{epoch}.pth\")\n","\n","if __name__ == \"__main__\":\n","    checkpoint_path = \"/content/drive/MyDrive/generator_epoch_280.pth\"\n","    train_gan(epochs=200, start_epoch=281, checkpoint_path=checkpoint_path, lr=0.000005)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD3ZE2M2FhFr","executionInfo":{"status":"ok","timestamp":1747201559189,"user_tz":-420,"elapsed":3711222,"user":{"displayName":"Tú Nguyễn","userId":"18020195782305815633"}},"outputId":"47dd1134-330d-467d-c521-19e2f7041ee7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:04<00:00, 111MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded checkpoint from /content/drive/MyDrive/generator_epoch_280.pth\n","Epoch [281/480] Batch [0/32] D Loss: 0.6932 G Loss: 4.2440\n","Epoch [281/480] Batch [10/32] D Loss: 0.6931 G Loss: 4.2888\n","Epoch [281/480] Batch [20/32] D Loss: 0.6928 G Loss: 4.4049\n","Epoch [281/480] Batch [30/32] D Loss: 0.6931 G Loss: 4.5884\n","Epoch [282/480] Batch [0/32] D Loss: 0.6930 G Loss: 3.8558\n","Epoch [282/480] Batch [10/32] D Loss: 0.6928 G Loss: 4.5542\n","Epoch [282/480] Batch [20/32] D Loss: 0.6927 G Loss: 4.4222\n","Epoch [282/480] Batch [30/32] D Loss: 0.6927 G Loss: 4.6977\n","Epoch [283/480] Batch [0/32] D Loss: 0.6924 G Loss: 4.3727\n","Epoch [283/480] Batch [10/32] D Loss: 0.6925 G Loss: 4.6222\n","Epoch [283/480] Batch [20/32] D Loss: 0.6926 G Loss: 4.4884\n","Epoch [283/480] Batch [30/32] D Loss: 0.6921 G Loss: 4.1030\n","Epoch [284/480] Batch [0/32] D Loss: 0.6920 G Loss: 4.2918\n","Epoch [284/480] Batch [10/32] D Loss: 0.6920 G Loss: 3.9667\n","Epoch [284/480] Batch [20/32] D Loss: 0.6922 G Loss: 4.6303\n","Epoch [284/480] Batch [30/32] D Loss: 0.6917 G Loss: 4.5289\n","Epoch [285/480] Batch [0/32] D Loss: 0.6919 G Loss: 4.5828\n","Epoch [285/480] Batch [10/32] D Loss: 0.6905 G Loss: 4.5520\n","Epoch [285/480] Batch [20/32] D Loss: 0.6912 G Loss: 4.1345\n","Epoch [285/480] Batch [30/32] D Loss: 0.6911 G Loss: 3.9165\n","Epoch [286/480] Batch [0/32] D Loss: 0.6907 G Loss: 4.8689\n","Epoch [286/480] Batch [10/32] D Loss: 0.6908 G Loss: 4.8327\n","Epoch [286/480] Batch [20/32] D Loss: 0.6929 G Loss: 5.2719\n","Epoch [286/480] Batch [30/32] D Loss: 0.6907 G Loss: 4.1778\n","Epoch [287/480] Batch [0/32] D Loss: 0.6918 G Loss: 4.1960\n","Epoch [287/480] Batch [10/32] D Loss: 0.6907 G Loss: 4.2325\n","Epoch [287/480] Batch [20/32] D Loss: 0.6900 G Loss: 4.4516\n","Epoch [287/480] Batch [30/32] D Loss: 0.6888 G Loss: 4.6701\n","Epoch [288/480] Batch [0/32] D Loss: 0.6870 G Loss: 4.1737\n","Epoch [288/480] Batch [10/32] D Loss: 0.6899 G Loss: 4.8772\n","Epoch [288/480] Batch [20/32] D Loss: 0.6886 G Loss: 3.9012\n","Epoch [288/480] Batch [30/32] D Loss: 0.6894 G Loss: 4.7390\n","Epoch [289/480] Batch [0/32] D Loss: 0.6904 G Loss: 4.9011\n","Epoch [289/480] Batch [10/32] D Loss: 0.6897 G Loss: 4.4395\n","Epoch [289/480] Batch [20/32] D Loss: 0.6892 G Loss: 5.0093\n","Epoch [289/480] Batch [30/32] D Loss: 0.6857 G Loss: 4.7080\n","Epoch [290/480] Batch [0/32] D Loss: 0.6840 G Loss: 5.2128\n","Epoch [290/480] Batch [10/32] D Loss: 0.6878 G Loss: 4.8591\n","Epoch [290/480] Batch [20/32] D Loss: 0.6869 G Loss: 4.3435\n","Epoch [290/480] Batch [30/32] D Loss: 0.6914 G Loss: 3.7532\n","Epoch [291/480] Batch [0/32] D Loss: 0.6881 G Loss: 3.8365\n","Epoch [291/480] Batch [10/32] D Loss: 0.6888 G Loss: 4.5754\n","Epoch [291/480] Batch [20/32] D Loss: 0.6837 G Loss: 4.7393\n","Epoch [291/480] Batch [30/32] D Loss: 0.6924 G Loss: 4.2157\n","Epoch [292/480] Batch [0/32] D Loss: 0.6883 G Loss: 4.1880\n","Epoch [292/480] Batch [10/32] D Loss: 0.6878 G Loss: 4.5031\n","Epoch [292/480] Batch [20/32] D Loss: 0.6838 G Loss: 4.1863\n","Epoch [292/480] Batch [30/32] D Loss: 0.6811 G Loss: 4.4434\n","Epoch [293/480] Batch [0/32] D Loss: 0.6794 G Loss: 4.1549\n","Epoch [293/480] Batch [10/32] D Loss: 0.6791 G Loss: 4.2336\n","Epoch [293/480] Batch [20/32] D Loss: 0.6873 G Loss: 3.8599\n","Epoch [293/480] Batch [30/32] D Loss: 0.6853 G Loss: 4.0795\n","Epoch [294/480] Batch [0/32] D Loss: 0.6778 G Loss: 4.4253\n","Epoch [294/480] Batch [10/32] D Loss: 0.6773 G Loss: 4.5486\n","Epoch [294/480] Batch [20/32] D Loss: 0.6923 G Loss: 4.4443\n","Epoch [294/480] Batch [30/32] D Loss: 0.6798 G Loss: 4.5278\n","Epoch [295/480] Batch [0/32] D Loss: 0.6828 G Loss: 5.5485\n","Epoch [295/480] Batch [10/32] D Loss: 0.6800 G Loss: 4.0765\n","Epoch [295/480] Batch [20/32] D Loss: 0.6894 G Loss: 4.0437\n","Epoch [295/480] Batch [30/32] D Loss: 0.6933 G Loss: 4.6644\n","Epoch [296/480] Batch [0/32] D Loss: 0.6750 G Loss: 4.3605\n","Epoch [296/480] Batch [10/32] D Loss: 0.6866 G Loss: 4.0958\n","Epoch [296/480] Batch [20/32] D Loss: 0.6789 G Loss: 4.4171\n","Epoch [296/480] Batch [30/32] D Loss: 0.6859 G Loss: 4.3288\n","Epoch [297/480] Batch [0/32] D Loss: 0.6847 G Loss: 4.9803\n","Epoch [297/480] Batch [10/32] D Loss: 0.6787 G Loss: 4.5348\n","Epoch [297/480] Batch [20/32] D Loss: 0.6794 G Loss: 4.5173\n","Epoch [297/480] Batch [30/32] D Loss: 0.6884 G Loss: 4.1495\n","Epoch [298/480] Batch [0/32] D Loss: 0.6913 G Loss: 4.4909\n","Epoch [298/480] Batch [10/32] D Loss: 0.6933 G Loss: 4.8854\n","Epoch [298/480] Batch [20/32] D Loss: 0.6898 G Loss: 4.6409\n","Epoch [298/480] Batch [30/32] D Loss: 0.6871 G Loss: 4.8493\n","Epoch [299/480] Batch [0/32] D Loss: 0.6800 G Loss: 4.3620\n","Epoch [299/480] Batch [10/32] D Loss: 0.6796 G Loss: 4.1412\n","Epoch [299/480] Batch [20/32] D Loss: 0.6935 G Loss: 4.0867\n","Epoch [299/480] Batch [30/32] D Loss: 0.6774 G Loss: 4.8134\n","Epoch [300/480] Batch [0/32] D Loss: 0.6848 G Loss: 5.1619\n","Epoch [300/480] Batch [10/32] D Loss: 0.6806 G Loss: 4.7795\n","Epoch [300/480] Batch [20/32] D Loss: 0.6912 G Loss: 4.2443\n","Epoch [300/480] Batch [30/32] D Loss: 0.6829 G Loss: 4.6567\n","Epoch [301/480] Batch [0/32] D Loss: 0.6770 G Loss: 4.6175\n","Epoch [301/480] Batch [10/32] D Loss: 0.6749 G Loss: 4.3645\n","Epoch [301/480] Batch [20/32] D Loss: 0.6891 G Loss: 4.1353\n","Epoch [301/480] Batch [30/32] D Loss: 0.6925 G Loss: 4.8198\n","Epoch [302/480] Batch [0/32] D Loss: 0.6733 G Loss: 4.2956\n","Epoch [302/480] Batch [10/32] D Loss: 0.6834 G Loss: 4.1409\n","Epoch [302/480] Batch [20/32] D Loss: 0.6898 G Loss: 4.1964\n","Epoch [302/480] Batch [30/32] D Loss: 0.6839 G Loss: 4.5060\n","Epoch [303/480] Batch [0/32] D Loss: 0.6886 G Loss: 4.5711\n","Epoch [303/480] Batch [10/32] D Loss: 0.6764 G Loss: 4.6048\n","Epoch [303/480] Batch [20/32] D Loss: 0.6811 G Loss: 4.1759\n","Epoch [303/480] Batch [30/32] D Loss: 0.6831 G Loss: 4.2529\n","Epoch [304/480] Batch [0/32] D Loss: 0.6866 G Loss: 4.3743\n","Epoch [304/480] Batch [10/32] D Loss: 0.6778 G Loss: 4.5883\n","Epoch [304/480] Batch [20/32] D Loss: 0.6780 G Loss: 4.6231\n","Epoch [304/480] Batch [30/32] D Loss: 0.6828 G Loss: 4.3087\n","Epoch [305/480] Batch [0/32] D Loss: 0.6826 G Loss: 3.7134\n","Epoch [305/480] Batch [10/32] D Loss: 0.6774 G Loss: 4.5652\n","Epoch [305/480] Batch [20/32] D Loss: 0.6874 G Loss: 4.2552\n","Epoch [305/480] Batch [30/32] D Loss: 0.6839 G Loss: 4.4718\n","Epoch [306/480] Batch [0/32] D Loss: 0.6834 G Loss: 4.6503\n","Epoch [306/480] Batch [10/32] D Loss: 0.6811 G Loss: 3.7216\n","Epoch [306/480] Batch [20/32] D Loss: 0.6791 G Loss: 3.9299\n","Epoch [306/480] Batch [30/32] D Loss: 0.6792 G Loss: 4.5394\n","Epoch [307/480] Batch [0/32] D Loss: 0.6918 G Loss: 4.2994\n","Epoch [307/480] Batch [10/32] D Loss: 0.6894 G Loss: 5.0068\n","Epoch [307/480] Batch [20/32] D Loss: 0.6841 G Loss: 5.1013\n","Epoch [307/480] Batch [30/32] D Loss: 0.6845 G Loss: 3.8303\n","Epoch [308/480] Batch [0/32] D Loss: 0.6900 G Loss: 4.7305\n","Epoch [308/480] Batch [10/32] D Loss: 0.6776 G Loss: 4.5799\n","Epoch [308/480] Batch [20/32] D Loss: 0.6794 G Loss: 4.4737\n","Epoch [308/480] Batch [30/32] D Loss: 0.6821 G Loss: 4.3141\n","Epoch [309/480] Batch [0/32] D Loss: 0.6875 G Loss: 4.1968\n","Epoch [309/480] Batch [10/32] D Loss: 0.6797 G Loss: 4.3280\n","Epoch [309/480] Batch [20/32] D Loss: 0.6819 G Loss: 4.7787\n","Epoch [309/480] Batch [30/32] D Loss: 0.6790 G Loss: 4.2783\n","Epoch [310/480] Batch [0/32] D Loss: 0.6920 G Loss: 4.5129\n","Epoch [310/480] Batch [10/32] D Loss: 0.6802 G Loss: 4.3326\n","Epoch [310/480] Batch [20/32] D Loss: 0.6785 G Loss: 3.9946\n","Epoch [310/480] Batch [30/32] D Loss: 0.6880 G Loss: 4.4189\n","Epoch [311/480] Batch [0/32] D Loss: 0.6819 G Loss: 4.3749\n","Epoch [311/480] Batch [10/32] D Loss: 0.6745 G Loss: 4.4299\n","Epoch [311/480] Batch [20/32] D Loss: 0.6947 G Loss: 4.2910\n","Epoch [311/480] Batch [30/32] D Loss: 0.6900 G Loss: 4.4000\n","Epoch [312/480] Batch [0/32] D Loss: 0.6837 G Loss: 4.5766\n","Epoch [312/480] Batch [10/32] D Loss: 0.6846 G Loss: 4.1635\n","Epoch [312/480] Batch [20/32] D Loss: 0.6919 G Loss: 4.1636\n","Epoch [312/480] Batch [30/32] D Loss: 0.6945 G Loss: 4.5451\n","Epoch [313/480] Batch [0/32] D Loss: 0.6759 G Loss: 4.9424\n","Epoch [313/480] Batch [10/32] D Loss: 0.6809 G Loss: 4.9755\n","Epoch [313/480] Batch [20/32] D Loss: 0.6918 G Loss: 4.3496\n","Epoch [313/480] Batch [30/32] D Loss: 0.6814 G Loss: 4.6896\n","Epoch [314/480] Batch [0/32] D Loss: 0.6673 G Loss: 3.9103\n","Epoch [314/480] Batch [10/32] D Loss: 0.6768 G Loss: 5.1615\n","Epoch [314/480] Batch [20/32] D Loss: 0.6876 G Loss: 4.6325\n","Epoch [314/480] Batch [30/32] D Loss: 0.6941 G Loss: 4.6354\n","Epoch [315/480] Batch [0/32] D Loss: 0.6822 G Loss: 4.1402\n","Epoch [315/480] Batch [10/32] D Loss: 0.6764 G Loss: 4.4178\n","Epoch [315/480] Batch [20/32] D Loss: 0.6829 G Loss: 4.6773\n","Epoch [315/480] Batch [30/32] D Loss: 0.6859 G Loss: 5.3511\n","Epoch [316/480] Batch [0/32] D Loss: 0.6792 G Loss: 4.5527\n","Epoch [316/480] Batch [10/32] D Loss: 0.6907 G Loss: 3.5457\n","Epoch [316/480] Batch [20/32] D Loss: 0.6825 G Loss: 4.1216\n","Epoch [316/480] Batch [30/32] D Loss: 0.6872 G Loss: 4.2738\n","Epoch [317/480] Batch [0/32] D Loss: 0.6887 G Loss: 4.2835\n","Epoch [317/480] Batch [10/32] D Loss: 0.6911 G Loss: 4.8647\n","Epoch [317/480] Batch [20/32] D Loss: 0.6855 G Loss: 3.9996\n","Epoch [317/480] Batch [30/32] D Loss: 0.6903 G Loss: 4.5797\n","Epoch [318/480] Batch [0/32] D Loss: 0.6865 G Loss: 4.1723\n","Epoch [318/480] Batch [10/32] D Loss: 0.6779 G Loss: 4.4090\n","Epoch [318/480] Batch [20/32] D Loss: 0.6880 G Loss: 3.8240\n","Epoch [318/480] Batch [30/32] D Loss: 0.6911 G Loss: 4.0856\n","Epoch [319/480] Batch [0/32] D Loss: 0.6793 G Loss: 4.2901\n","Epoch [319/480] Batch [10/32] D Loss: 0.6911 G Loss: 4.7443\n","Epoch [319/480] Batch [20/32] D Loss: 0.6837 G Loss: 4.4583\n","Epoch [319/480] Batch [30/32] D Loss: 0.6959 G Loss: 3.7570\n","Epoch [320/480] Batch [0/32] D Loss: 0.6799 G Loss: 4.1167\n","Epoch [320/480] Batch [10/32] D Loss: 0.6839 G Loss: 4.3873\n","Epoch [320/480] Batch [20/32] D Loss: 0.6746 G Loss: 3.6244\n","Epoch [320/480] Batch [30/32] D Loss: 0.6761 G Loss: 4.9175\n","Epoch [321/480] Batch [0/32] D Loss: 0.6893 G Loss: 4.4196\n","Epoch [321/480] Batch [10/32] D Loss: 0.6987 G Loss: 4.4815\n","Epoch [321/480] Batch [20/32] D Loss: 0.6846 G Loss: 4.2391\n","Epoch [321/480] Batch [30/32] D Loss: 0.6951 G Loss: 4.1441\n","Epoch [322/480] Batch [0/32] D Loss: 0.6814 G Loss: 4.5178\n","Epoch [322/480] Batch [10/32] D Loss: 0.6813 G Loss: 4.1910\n","Epoch [322/480] Batch [20/32] D Loss: 0.6759 G Loss: 4.6864\n","Epoch [322/480] Batch [30/32] D Loss: 0.6930 G Loss: 4.2478\n","Epoch [323/480] Batch [0/32] D Loss: 0.6797 G Loss: 4.4493\n","Epoch [323/480] Batch [10/32] D Loss: 0.6799 G Loss: 5.2618\n","Epoch [323/480] Batch [20/32] D Loss: 0.6942 G Loss: 4.8756\n","Epoch [323/480] Batch [30/32] D Loss: 0.6917 G Loss: 4.1824\n","Epoch [324/480] Batch [0/32] D Loss: 0.6913 G Loss: 3.9278\n","Epoch [324/480] Batch [10/32] D Loss: 0.6715 G Loss: 4.8615\n","Epoch [324/480] Batch [20/32] D Loss: 0.6778 G Loss: 4.2817\n","Epoch [324/480] Batch [30/32] D Loss: 0.6817 G Loss: 4.0988\n","Epoch [325/480] Batch [0/32] D Loss: 0.6869 G Loss: 4.2070\n","Epoch [325/480] Batch [10/32] D Loss: 0.6863 G Loss: 3.9304\n","Epoch [325/480] Batch [20/32] D Loss: 0.6705 G Loss: 4.3245\n","Epoch [325/480] Batch [30/32] D Loss: 0.6866 G Loss: 4.0276\n","Epoch [326/480] Batch [0/32] D Loss: 0.6983 G Loss: 4.7711\n","Epoch [326/480] Batch [10/32] D Loss: 0.6922 G Loss: 4.8028\n","Epoch [326/480] Batch [20/32] D Loss: 0.6830 G Loss: 4.5907\n","Epoch [326/480] Batch [30/32] D Loss: 0.6769 G Loss: 4.1711\n","Epoch [327/480] Batch [0/32] D Loss: 0.6949 G Loss: 5.0238\n","Epoch [327/480] Batch [10/32] D Loss: 0.6836 G Loss: 4.2135\n","Epoch [327/480] Batch [20/32] D Loss: 0.6798 G Loss: 3.9937\n","Epoch [327/480] Batch [30/32] D Loss: 0.7032 G Loss: 4.3817\n","Epoch [328/480] Batch [0/32] D Loss: 0.6989 G Loss: 4.1589\n","Epoch [328/480] Batch [10/32] D Loss: 0.6923 G Loss: 4.4039\n","Epoch [328/480] Batch [20/32] D Loss: 0.6776 G Loss: 4.2965\n","Epoch [328/480] Batch [30/32] D Loss: 0.6902 G Loss: 4.7708\n","Epoch [329/480] Batch [0/32] D Loss: 0.6899 G Loss: 4.6535\n","Epoch [329/480] Batch [10/32] D Loss: 0.6806 G Loss: 3.9573\n","Epoch [329/480] Batch [20/32] D Loss: 0.6880 G Loss: 3.9159\n","Epoch [329/480] Batch [30/32] D Loss: 0.6912 G Loss: 4.4776\n","Epoch [330/480] Batch [0/32] D Loss: 0.6910 G Loss: 4.5814\n","Epoch [330/480] Batch [10/32] D Loss: 0.6811 G Loss: 3.9182\n","Epoch [330/480] Batch [20/32] D Loss: 0.6847 G Loss: 4.2116\n","Epoch [330/480] Batch [30/32] D Loss: 0.6735 G Loss: 4.2945\n","Epoch [331/480] Batch [0/32] D Loss: 0.6875 G Loss: 4.5233\n","Epoch [331/480] Batch [10/32] D Loss: 0.6822 G Loss: 4.1339\n","Epoch [331/480] Batch [20/32] D Loss: 0.6896 G Loss: 4.2680\n","Epoch [331/480] Batch [30/32] D Loss: 0.6786 G Loss: 4.4269\n","Epoch [332/480] Batch [0/32] D Loss: 0.6705 G Loss: 4.0428\n","Epoch [332/480] Batch [10/32] D Loss: 0.6869 G Loss: 4.0507\n","Epoch [332/480] Batch [20/32] D Loss: 0.6671 G Loss: 4.3309\n","Epoch [332/480] Batch [30/32] D Loss: 0.6732 G Loss: 4.5338\n","Epoch [333/480] Batch [0/32] D Loss: 0.6880 G Loss: 4.3557\n","Epoch [333/480] Batch [10/32] D Loss: 0.6862 G Loss: 3.7436\n","Epoch [333/480] Batch [20/32] D Loss: 0.6725 G Loss: 4.5980\n","Epoch [333/480] Batch [30/32] D Loss: 0.6947 G Loss: 4.2977\n","Epoch [334/480] Batch [0/32] D Loss: 0.6890 G Loss: 3.9223\n","Epoch [334/480] Batch [10/32] D Loss: 0.6941 G Loss: 3.8866\n","Epoch [334/480] Batch [20/32] D Loss: 0.6877 G Loss: 4.9655\n","Epoch [334/480] Batch [30/32] D Loss: 0.6801 G Loss: 4.3621\n","Epoch [335/480] Batch [0/32] D Loss: 0.6900 G Loss: 4.3582\n","Epoch [335/480] Batch [10/32] D Loss: 0.6864 G Loss: 4.6171\n","Epoch [335/480] Batch [20/32] D Loss: 0.6784 G Loss: 5.2437\n","Epoch [335/480] Batch [30/32] D Loss: 0.6684 G Loss: 4.3257\n","Epoch [336/480] Batch [0/32] D Loss: 0.6861 G Loss: 4.6988\n","Epoch [336/480] Batch [10/32] D Loss: 0.6776 G Loss: 4.2652\n","Epoch [336/480] Batch [20/32] D Loss: 0.6796 G Loss: 3.7577\n","Epoch [336/480] Batch [30/32] D Loss: 0.6793 G Loss: 4.5277\n","Epoch [337/480] Batch [0/32] D Loss: 0.6865 G Loss: 4.4638\n","Epoch [337/480] Batch [10/32] D Loss: 0.6955 G Loss: 4.2643\n","Epoch [337/480] Batch [20/32] D Loss: 0.6849 G Loss: 4.2915\n","Epoch [337/480] Batch [30/32] D Loss: 0.6748 G Loss: 4.8224\n","Epoch [338/480] Batch [0/32] D Loss: 0.6914 G Loss: 3.7820\n","Epoch [338/480] Batch [10/32] D Loss: 0.6814 G Loss: 4.3000\n","Epoch [338/480] Batch [20/32] D Loss: 0.6649 G Loss: 4.7525\n","Epoch [338/480] Batch [30/32] D Loss: 0.6859 G Loss: 4.3940\n","Epoch [339/480] Batch [0/32] D Loss: 0.6878 G Loss: 4.4082\n","Epoch [339/480] Batch [10/32] D Loss: 0.6874 G Loss: 4.4997\n","Epoch [339/480] Batch [20/32] D Loss: 0.6996 G Loss: 3.9471\n","Epoch [339/480] Batch [30/32] D Loss: 0.6712 G Loss: 4.5110\n","Epoch [340/480] Batch [0/32] D Loss: 0.6931 G Loss: 3.9461\n","Epoch [340/480] Batch [10/32] D Loss: 0.6737 G Loss: 4.8431\n","Epoch [340/480] Batch [20/32] D Loss: 0.6772 G Loss: 4.7583\n","Epoch [340/480] Batch [30/32] D Loss: 0.6857 G Loss: 3.8890\n","Epoch [341/480] Batch [0/32] D Loss: 0.6853 G Loss: 4.4437\n","Epoch [341/480] Batch [10/32] D Loss: 0.6840 G Loss: 3.9048\n","Epoch [341/480] Batch [20/32] D Loss: 0.6902 G Loss: 4.7437\n","Epoch [341/480] Batch [30/32] D Loss: 0.6963 G Loss: 4.3443\n","Epoch [342/480] Batch [0/32] D Loss: 0.6876 G Loss: 4.9892\n","Epoch [342/480] Batch [10/32] D Loss: 0.6808 G Loss: 4.5678\n","Epoch [342/480] Batch [20/32] D Loss: 0.6826 G Loss: 4.8617\n","Epoch [342/480] Batch [30/32] D Loss: 0.6856 G Loss: 4.8435\n","Epoch [343/480] Batch [0/32] D Loss: 0.6836 G Loss: 4.5894\n","Epoch [343/480] Batch [10/32] D Loss: 0.6898 G Loss: 4.1998\n","Epoch [343/480] Batch [20/32] D Loss: 0.6946 G Loss: 4.4638\n","Epoch [343/480] Batch [30/32] D Loss: 0.6887 G Loss: 3.7350\n","Epoch [344/480] Batch [0/32] D Loss: 0.6828 G Loss: 3.9088\n","Epoch [344/480] Batch [10/32] D Loss: 0.6799 G Loss: 4.8765\n","Epoch [344/480] Batch [20/32] D Loss: 0.6855 G Loss: 4.9365\n","Epoch [344/480] Batch [30/32] D Loss: 0.6850 G Loss: 4.1084\n","Epoch [345/480] Batch [0/32] D Loss: 0.6798 G Loss: 4.4008\n","Epoch [345/480] Batch [10/32] D Loss: 0.6921 G Loss: 4.1922\n","Epoch [345/480] Batch [20/32] D Loss: 0.6947 G Loss: 4.4187\n","Epoch [345/480] Batch [30/32] D Loss: 0.6837 G Loss: 4.1230\n","Epoch [346/480] Batch [0/32] D Loss: 0.6908 G Loss: 3.9142\n","Epoch [346/480] Batch [10/32] D Loss: 0.6810 G Loss: 4.5529\n","Epoch [346/480] Batch [20/32] D Loss: 0.6788 G Loss: 4.8759\n","Epoch [346/480] Batch [30/32] D Loss: 0.6858 G Loss: 4.2945\n","Epoch [347/480] Batch [0/32] D Loss: 0.6785 G Loss: 3.7734\n","Epoch [347/480] Batch [10/32] D Loss: 0.6972 G Loss: 4.5478\n","Epoch [347/480] Batch [20/32] D Loss: 0.6697 G Loss: 4.3509\n","Epoch [347/480] Batch [30/32] D Loss: 0.7074 G Loss: 4.7655\n","Epoch [348/480] Batch [0/32] D Loss: 0.6774 G Loss: 4.5687\n","Epoch [348/480] Batch [10/32] D Loss: 0.6898 G Loss: 3.7524\n","Epoch [348/480] Batch [20/32] D Loss: 0.7009 G Loss: 4.7110\n","Epoch [348/480] Batch [30/32] D Loss: 0.6776 G Loss: 4.6572\n","Epoch [349/480] Batch [0/32] D Loss: 0.6768 G Loss: 3.5622\n","Epoch [349/480] Batch [10/32] D Loss: 0.6793 G Loss: 4.2111\n","Epoch [349/480] Batch [20/32] D Loss: 0.6875 G Loss: 4.1321\n","Epoch [349/480] Batch [30/32] D Loss: 0.6813 G Loss: 4.8069\n","Epoch [350/480] Batch [0/32] D Loss: 0.6741 G Loss: 4.2161\n","Epoch [350/480] Batch [10/32] D Loss: 0.6660 G Loss: 4.5694\n","Epoch [350/480] Batch [20/32] D Loss: 0.6877 G Loss: 5.2139\n","Epoch [350/480] Batch [30/32] D Loss: 0.6702 G Loss: 5.2193\n","Epoch [351/480] Batch [0/32] D Loss: 0.6822 G Loss: 5.0568\n","Epoch [351/480] Batch [10/32] D Loss: 0.6665 G Loss: 4.1591\n","Epoch [351/480] Batch [20/32] D Loss: 0.6973 G Loss: 4.3681\n","Epoch [351/480] Batch [30/32] D Loss: 0.6859 G Loss: 4.5833\n","Epoch [352/480] Batch [0/32] D Loss: 0.6779 G Loss: 4.4379\n","Epoch [352/480] Batch [10/32] D Loss: 0.6856 G Loss: 4.4527\n","Epoch [352/480] Batch [20/32] D Loss: 0.6750 G Loss: 5.1155\n","Epoch [352/480] Batch [30/32] D Loss: 0.6767 G Loss: 4.7776\n","Epoch [353/480] Batch [0/32] D Loss: 0.6715 G Loss: 5.0272\n","Epoch [353/480] Batch [10/32] D Loss: 0.6799 G Loss: 3.8837\n","Epoch [353/480] Batch [20/32] D Loss: 0.6901 G Loss: 4.2962\n","Epoch [353/480] Batch [30/32] D Loss: 0.6718 G Loss: 4.3895\n","Epoch [354/480] Batch [0/32] D Loss: 0.6925 G Loss: 4.2815\n","Epoch [354/480] Batch [10/32] D Loss: 0.6878 G Loss: 4.2097\n","Epoch [354/480] Batch [20/32] D Loss: 0.6917 G Loss: 3.9064\n","Epoch [354/480] Batch [30/32] D Loss: 0.6747 G Loss: 4.5333\n","Epoch [355/480] Batch [0/32] D Loss: 0.6753 G Loss: 4.4217\n","Epoch [355/480] Batch [10/32] D Loss: 0.6715 G Loss: 4.5835\n","Epoch [355/480] Batch [20/32] D Loss: 0.6994 G Loss: 4.0928\n","Epoch [355/480] Batch [30/32] D Loss: 0.6878 G Loss: 4.4019\n","Epoch [356/480] Batch [0/32] D Loss: 0.6855 G Loss: 4.3323\n","Epoch [356/480] Batch [10/32] D Loss: 0.6861 G Loss: 4.4842\n","Epoch [356/480] Batch [20/32] D Loss: 0.6812 G Loss: 5.3103\n","Epoch [356/480] Batch [30/32] D Loss: 0.6726 G Loss: 4.1638\n","Epoch [357/480] Batch [0/32] D Loss: 0.6799 G Loss: 5.4924\n","Epoch [357/480] Batch [10/32] D Loss: 0.6906 G Loss: 4.2287\n","Epoch [357/480] Batch [20/32] D Loss: 0.6700 G Loss: 4.3623\n","Epoch [357/480] Batch [30/32] D Loss: 0.6702 G Loss: 4.5712\n","Epoch [358/480] Batch [0/32] D Loss: 0.6653 G Loss: 4.3458\n","Epoch [358/480] Batch [10/32] D Loss: 0.6806 G Loss: 4.5257\n","Epoch [358/480] Batch [20/32] D Loss: 0.6889 G Loss: 4.1241\n","Epoch [358/480] Batch [30/32] D Loss: 0.6872 G Loss: 4.4433\n","Epoch [359/480] Batch [0/32] D Loss: 0.6877 G Loss: 4.8001\n","Epoch [359/480] Batch [10/32] D Loss: 0.6798 G Loss: 4.6952\n","Epoch [359/480] Batch [20/32] D Loss: 0.6843 G Loss: 3.8494\n","Epoch [359/480] Batch [30/32] D Loss: 0.6798 G Loss: 4.5919\n","Epoch [360/480] Batch [0/32] D Loss: 0.6915 G Loss: 5.0268\n","Epoch [360/480] Batch [10/32] D Loss: 0.6782 G Loss: 4.3958\n","Epoch [360/480] Batch [20/32] D Loss: 0.6574 G Loss: 4.6680\n","Epoch [360/480] Batch [30/32] D Loss: 0.6812 G Loss: 3.9792\n","Epoch [361/480] Batch [0/32] D Loss: 0.6688 G Loss: 4.2491\n","Epoch [361/480] Batch [10/32] D Loss: 0.6953 G Loss: 3.9616\n","Epoch [361/480] Batch [20/32] D Loss: 0.6705 G Loss: 4.6564\n","Epoch [361/480] Batch [30/32] D Loss: 0.6869 G Loss: 3.8411\n","Epoch [362/480] Batch [0/32] D Loss: 0.6772 G Loss: 4.6583\n","Epoch [362/480] Batch [10/32] D Loss: 0.6777 G Loss: 3.9682\n","Epoch [362/480] Batch [20/32] D Loss: 0.6784 G Loss: 4.1404\n","Epoch [362/480] Batch [30/32] D Loss: 0.6892 G Loss: 4.0087\n","Epoch [363/480] Batch [0/32] D Loss: 0.6882 G Loss: 4.3213\n","Epoch [363/480] Batch [10/32] D Loss: 0.6833 G Loss: 4.8360\n","Epoch [363/480] Batch [20/32] D Loss: 0.6800 G Loss: 4.1429\n","Epoch [363/480] Batch [30/32] D Loss: 0.6921 G Loss: 4.5800\n","Epoch [364/480] Batch [0/32] D Loss: 0.6921 G Loss: 4.4655\n","Epoch [364/480] Batch [10/32] D Loss: 0.6781 G Loss: 4.6440\n","Epoch [364/480] Batch [20/32] D Loss: 0.6781 G Loss: 3.9167\n","Epoch [364/480] Batch [30/32] D Loss: 0.6773 G Loss: 4.6153\n","Epoch [365/480] Batch [0/32] D Loss: 0.6729 G Loss: 4.4863\n","Epoch [365/480] Batch [10/32] D Loss: 0.6726 G Loss: 4.4147\n","Epoch [365/480] Batch [20/32] D Loss: 0.6776 G Loss: 4.0742\n","Epoch [365/480] Batch [30/32] D Loss: 0.6772 G Loss: 3.8364\n","Epoch [366/480] Batch [0/32] D Loss: 0.6963 G Loss: 4.3244\n","Epoch [366/480] Batch [10/32] D Loss: 0.6687 G Loss: 4.4372\n","Epoch [366/480] Batch [20/32] D Loss: 0.6972 G Loss: 4.3775\n","Epoch [366/480] Batch [30/32] D Loss: 0.6927 G Loss: 3.7478\n","Epoch [367/480] Batch [0/32] D Loss: 0.6648 G Loss: 4.2644\n","Epoch [367/480] Batch [10/32] D Loss: 0.6863 G Loss: 4.3107\n","Epoch [367/480] Batch [20/32] D Loss: 0.6829 G Loss: 4.8005\n","Epoch [367/480] Batch [30/32] D Loss: 0.6770 G Loss: 4.7418\n","Epoch [368/480] Batch [0/32] D Loss: 0.6845 G Loss: 4.0280\n","Epoch [368/480] Batch [10/32] D Loss: 0.6755 G Loss: 4.2764\n","Epoch [368/480] Batch [20/32] D Loss: 0.6711 G Loss: 4.8092\n","Epoch [368/480] Batch [30/32] D Loss: 0.6964 G Loss: 4.2365\n","Epoch [369/480] Batch [0/32] D Loss: 0.6833 G Loss: 4.6806\n","Epoch [369/480] Batch [10/32] D Loss: 0.6813 G Loss: 4.4271\n","Epoch [369/480] Batch [20/32] D Loss: 0.6966 G Loss: 4.2438\n","Epoch [369/480] Batch [30/32] D Loss: 0.6774 G Loss: 4.6584\n","Epoch [370/480] Batch [0/32] D Loss: 0.6921 G Loss: 3.8135\n","Epoch [370/480] Batch [10/32] D Loss: 0.6700 G Loss: 4.3478\n","Epoch [370/480] Batch [20/32] D Loss: 0.6893 G Loss: 3.9951\n","Epoch [370/480] Batch [30/32] D Loss: 0.6786 G Loss: 4.7403\n","Epoch [371/480] Batch [0/32] D Loss: 0.6889 G Loss: 3.7957\n","Epoch [371/480] Batch [10/32] D Loss: 0.6827 G Loss: 4.0215\n","Epoch [371/480] Batch [20/32] D Loss: 0.6664 G Loss: 4.0641\n","Epoch [371/480] Batch [30/32] D Loss: 0.6638 G Loss: 4.5454\n","Epoch [372/480] Batch [0/32] D Loss: 0.6756 G Loss: 4.4535\n","Epoch [372/480] Batch [10/32] D Loss: 0.6820 G Loss: 4.5035\n","Epoch [372/480] Batch [20/32] D Loss: 0.6780 G Loss: 4.1479\n","Epoch [372/480] Batch [30/32] D Loss: 0.6820 G Loss: 4.5106\n","Epoch [373/480] Batch [0/32] D Loss: 0.6884 G Loss: 4.8071\n","Epoch [373/480] Batch [10/32] D Loss: 0.6778 G Loss: 4.4338\n","Epoch [373/480] Batch [20/32] D Loss: 0.6726 G Loss: 4.7334\n","Epoch [373/480] Batch [30/32] D Loss: 0.6771 G Loss: 3.8247\n","Epoch [374/480] Batch [0/32] D Loss: 0.6733 G Loss: 4.3058\n","Epoch [374/480] Batch [10/32] D Loss: 0.6807 G Loss: 3.8729\n","Epoch [374/480] Batch [20/32] D Loss: 0.6961 G Loss: 4.6537\n","Epoch [374/480] Batch [30/32] D Loss: 0.6825 G Loss: 4.4039\n","Epoch [375/480] Batch [0/32] D Loss: 0.6907 G Loss: 4.4698\n","Epoch [375/480] Batch [10/32] D Loss: 0.6588 G Loss: 3.9301\n","Epoch [375/480] Batch [20/32] D Loss: 0.6914 G Loss: 4.3447\n","Epoch [375/480] Batch [30/32] D Loss: 0.6740 G Loss: 4.2608\n","Epoch [376/480] Batch [0/32] D Loss: 0.6868 G Loss: 4.3635\n","Epoch [376/480] Batch [10/32] D Loss: 0.6817 G Loss: 4.2044\n","Epoch [376/480] Batch [20/32] D Loss: 0.6778 G Loss: 5.1527\n","Epoch [376/480] Batch [30/32] D Loss: 0.6938 G Loss: 4.3968\n","Epoch [377/480] Batch [0/32] D Loss: 0.6831 G Loss: 3.9289\n","Epoch [377/480] Batch [10/32] D Loss: 0.6638 G Loss: 4.8138\n","Epoch [377/480] Batch [20/32] D Loss: 0.6815 G Loss: 4.3149\n","Epoch [377/480] Batch [30/32] D Loss: 0.6960 G Loss: 4.2267\n","Epoch [378/480] Batch [0/32] D Loss: 0.6721 G Loss: 4.2233\n","Epoch [378/480] Batch [10/32] D Loss: 0.7029 G Loss: 4.5172\n","Epoch [378/480] Batch [20/32] D Loss: 0.6872 G Loss: 4.7918\n","Epoch [378/480] Batch [30/32] D Loss: 0.6804 G Loss: 4.5480\n","Epoch [379/480] Batch [0/32] D Loss: 0.6839 G Loss: 3.5028\n","Epoch [379/480] Batch [10/32] D Loss: 0.6746 G Loss: 4.5534\n","Epoch [379/480] Batch [20/32] D Loss: 0.6867 G Loss: 4.3894\n","Epoch [379/480] Batch [30/32] D Loss: 0.6766 G Loss: 4.2693\n","Epoch [380/480] Batch [0/32] D Loss: 0.6799 G Loss: 4.1423\n","Epoch [380/480] Batch [10/32] D Loss: 0.6736 G Loss: 4.1550\n","Epoch [380/480] Batch [20/32] D Loss: 0.6638 G Loss: 4.5827\n","Epoch [380/480] Batch [30/32] D Loss: 0.6769 G Loss: 4.1613\n","Epoch [381/480] Batch [0/32] D Loss: 0.6797 G Loss: 3.9877\n","Epoch [381/480] Batch [10/32] D Loss: 0.6920 G Loss: 4.3483\n","Epoch [381/480] Batch [20/32] D Loss: 0.6870 G Loss: 4.7744\n","Epoch [381/480] Batch [30/32] D Loss: 0.6870 G Loss: 4.0949\n","Epoch [382/480] Batch [0/32] D Loss: 0.6715 G Loss: 4.7317\n","Epoch [382/480] Batch [10/32] D Loss: 0.6799 G Loss: 4.3891\n","Epoch [382/480] Batch [20/32] D Loss: 0.6815 G Loss: 4.2768\n","Epoch [382/480] Batch [30/32] D Loss: 0.7000 G Loss: 4.3052\n","Epoch [383/480] Batch [0/32] D Loss: 0.6837 G Loss: 4.6831\n","Epoch [383/480] Batch [10/32] D Loss: 0.6714 G Loss: 4.0383\n","Epoch [383/480] Batch [20/32] D Loss: 0.6729 G Loss: 4.6238\n","Epoch [383/480] Batch [30/32] D Loss: 0.6778 G Loss: 4.2125\n","Epoch [384/480] Batch [0/32] D Loss: 0.6681 G Loss: 4.2329\n","Epoch [384/480] Batch [10/32] D Loss: 0.6901 G Loss: 4.0071\n","Epoch [384/480] Batch [20/32] D Loss: 0.6729 G Loss: 4.1885\n","Epoch [384/480] Batch [30/32] D Loss: 0.6884 G Loss: 4.6471\n","Epoch [385/480] Batch [0/32] D Loss: 0.6764 G Loss: 4.0871\n","Epoch [385/480] Batch [10/32] D Loss: 0.6650 G Loss: 4.4057\n","Epoch [385/480] Batch [20/32] D Loss: 0.6632 G Loss: 4.6223\n","Epoch [385/480] Batch [30/32] D Loss: 0.6766 G Loss: 4.3897\n","Epoch [386/480] Batch [0/32] D Loss: 0.6966 G Loss: 4.0345\n","Epoch [386/480] Batch [10/32] D Loss: 0.6757 G Loss: 4.1604\n","Epoch [386/480] Batch [20/32] D Loss: 0.6719 G Loss: 4.6257\n","Epoch [386/480] Batch [30/32] D Loss: 0.6900 G Loss: 4.0996\n","Epoch [387/480] Batch [0/32] D Loss: 0.6833 G Loss: 4.4578\n","Epoch [387/480] Batch [10/32] D Loss: 0.6930 G Loss: 4.4010\n","Epoch [387/480] Batch [20/32] D Loss: 0.6938 G Loss: 4.1515\n","Epoch [387/480] Batch [30/32] D Loss: 0.6735 G Loss: 4.5690\n","Epoch [388/480] Batch [0/32] D Loss: 0.6821 G Loss: 4.3418\n","Epoch [388/480] Batch [10/32] D Loss: 0.6809 G Loss: 3.9554\n","Epoch [388/480] Batch [20/32] D Loss: 0.6780 G Loss: 4.2989\n","Epoch [388/480] Batch [30/32] D Loss: 0.6766 G Loss: 4.3140\n","Epoch [389/480] Batch [0/32] D Loss: 0.6827 G Loss: 4.6714\n","Epoch [389/480] Batch [10/32] D Loss: 0.6876 G Loss: 4.3616\n","Epoch [389/480] Batch [20/32] D Loss: 0.6951 G Loss: 4.0243\n","Epoch [389/480] Batch [30/32] D Loss: 0.6787 G Loss: 4.2183\n","Epoch [390/480] Batch [0/32] D Loss: 0.6791 G Loss: 4.3804\n","Epoch [390/480] Batch [10/32] D Loss: 0.6724 G Loss: 4.3604\n","Epoch [390/480] Batch [20/32] D Loss: 0.6741 G Loss: 4.4798\n","Epoch [390/480] Batch [30/32] D Loss: 0.6884 G Loss: 4.4475\n","Epoch [391/480] Batch [0/32] D Loss: 0.6807 G Loss: 4.2939\n","Epoch [391/480] Batch [10/32] D Loss: 0.6798 G Loss: 4.4530\n","Epoch [391/480] Batch [20/32] D Loss: 0.7033 G Loss: 4.2670\n","Epoch [391/480] Batch [30/32] D Loss: 0.6800 G Loss: 4.1670\n","Epoch [392/480] Batch [0/32] D Loss: 0.6821 G Loss: 4.1228\n","Epoch [392/480] Batch [10/32] D Loss: 0.6799 G Loss: 4.7647\n","Epoch [392/480] Batch [20/32] D Loss: 0.7040 G Loss: 3.8807\n","Epoch [392/480] Batch [30/32] D Loss: 0.6719 G Loss: 3.7580\n","Epoch [393/480] Batch [0/32] D Loss: 0.6796 G Loss: 4.4131\n","Epoch [393/480] Batch [10/32] D Loss: 0.6822 G Loss: 4.2086\n","Epoch [393/480] Batch [20/32] D Loss: 0.6916 G Loss: 4.1274\n","Epoch [393/480] Batch [30/32] D Loss: 0.6815 G Loss: 4.5300\n","Epoch [394/480] Batch [0/32] D Loss: 0.6878 G Loss: 4.4490\n","Epoch [394/480] Batch [10/32] D Loss: 0.6842 G Loss: 4.3481\n","Epoch [394/480] Batch [20/32] D Loss: 0.6847 G Loss: 4.1364\n","Epoch [394/480] Batch [30/32] D Loss: 0.6726 G Loss: 3.6098\n","Epoch [395/480] Batch [0/32] D Loss: 0.6679 G Loss: 4.2275\n","Epoch [395/480] Batch [10/32] D Loss: 0.6984 G Loss: 4.5011\n","Epoch [395/480] Batch [20/32] D Loss: 0.6908 G Loss: 4.9288\n","Epoch [395/480] Batch [30/32] D Loss: 0.6746 G Loss: 4.6597\n","Epoch [396/480] Batch [0/32] D Loss: 0.6747 G Loss: 4.2602\n","Epoch [396/480] Batch [10/32] D Loss: 0.6821 G Loss: 4.2339\n","Epoch [396/480] Batch [20/32] D Loss: 0.6780 G Loss: 4.5201\n","Epoch [396/480] Batch [30/32] D Loss: 0.6849 G Loss: 5.2402\n","Epoch [397/480] Batch [0/32] D Loss: 0.6901 G Loss: 4.7447\n","Epoch [397/480] Batch [10/32] D Loss: 0.6834 G Loss: 4.6603\n","Epoch [397/480] Batch [20/32] D Loss: 0.7016 G Loss: 3.7048\n","Epoch [397/480] Batch [30/32] D Loss: 0.6895 G Loss: 4.1028\n","Epoch [398/480] Batch [0/32] D Loss: 0.7019 G Loss: 4.1583\n","Epoch [398/480] Batch [10/32] D Loss: 0.6785 G Loss: 4.0130\n","Epoch [398/480] Batch [20/32] D Loss: 0.6746 G Loss: 4.3719\n","Epoch [398/480] Batch [30/32] D Loss: 0.6729 G Loss: 4.0264\n","Epoch [399/480] Batch [0/32] D Loss: 0.6837 G Loss: 4.2196\n","Epoch [399/480] Batch [10/32] D Loss: 0.6746 G Loss: 4.1937\n","Epoch [399/480] Batch [20/32] D Loss: 0.6772 G Loss: 4.7129\n","Epoch [399/480] Batch [30/32] D Loss: 0.6620 G Loss: 4.3741\n","Epoch [400/480] Batch [0/32] D Loss: 0.6795 G Loss: 4.6271\n","Epoch [400/480] Batch [10/32] D Loss: 0.6914 G Loss: 4.4506\n","Epoch [400/480] Batch [20/32] D Loss: 0.6769 G Loss: 4.3263\n","Epoch [400/480] Batch [30/32] D Loss: 0.6849 G Loss: 4.4160\n","Epoch [401/480] Batch [0/32] D Loss: 0.6838 G Loss: 4.1129\n","Epoch [401/480] Batch [10/32] D Loss: 0.6901 G Loss: 4.0097\n","Epoch [401/480] Batch [20/32] D Loss: 0.6825 G Loss: 4.4854\n","Epoch [401/480] Batch [30/32] D Loss: 0.6863 G Loss: 4.4436\n","Epoch [402/480] Batch [0/32] D Loss: 0.6677 G Loss: 4.4759\n","Epoch [402/480] Batch [10/32] D Loss: 0.6718 G Loss: 4.3443\n","Epoch [402/480] Batch [20/32] D Loss: 0.6703 G Loss: 3.7818\n","Epoch [402/480] Batch [30/32] D Loss: 0.6897 G Loss: 3.7144\n","Epoch [403/480] Batch [0/32] D Loss: 0.6738 G Loss: 4.7884\n","Epoch [403/480] Batch [10/32] D Loss: 0.6747 G Loss: 4.5020\n","Epoch [403/480] Batch [20/32] D Loss: 0.6752 G Loss: 4.6226\n","Epoch [403/480] Batch [30/32] D Loss: 0.6833 G Loss: 4.4803\n","Epoch [404/480] Batch [0/32] D Loss: 0.6821 G Loss: 3.8335\n","Epoch [404/480] Batch [10/32] D Loss: 0.6766 G Loss: 4.1992\n","Epoch [404/480] Batch [20/32] D Loss: 0.6785 G Loss: 4.5072\n","Epoch [404/480] Batch [30/32] D Loss: 0.6853 G Loss: 4.8782\n","Epoch [405/480] Batch [0/32] D Loss: 0.6892 G Loss: 4.0199\n","Epoch [405/480] Batch [10/32] D Loss: 0.6707 G Loss: 4.6889\n","Epoch [405/480] Batch [20/32] D Loss: 0.6891 G Loss: 4.3911\n","Epoch [405/480] Batch [30/32] D Loss: 0.6905 G Loss: 4.7159\n","Epoch [406/480] Batch [0/32] D Loss: 0.6788 G Loss: 4.5162\n","Epoch [406/480] Batch [10/32] D Loss: 0.6686 G Loss: 3.9290\n","Epoch [406/480] Batch [20/32] D Loss: 0.6896 G Loss: 4.2794\n","Epoch [406/480] Batch [30/32] D Loss: 0.6821 G Loss: 4.2753\n","Epoch [407/480] Batch [0/32] D Loss: 0.6745 G Loss: 4.2457\n","Epoch [407/480] Batch [10/32] D Loss: 0.6571 G Loss: 4.1785\n","Epoch [407/480] Batch [20/32] D Loss: 0.6790 G Loss: 3.9697\n","Epoch [407/480] Batch [30/32] D Loss: 0.6750 G Loss: 4.1853\n","Epoch [408/480] Batch [0/32] D Loss: 0.6866 G Loss: 3.7749\n","Epoch [408/480] Batch [10/32] D Loss: 0.6795 G Loss: 4.5774\n","Epoch [408/480] Batch [20/32] D Loss: 0.6750 G Loss: 4.4239\n","Epoch [408/480] Batch [30/32] D Loss: 0.6741 G Loss: 3.9264\n","Epoch [409/480] Batch [0/32] D Loss: 0.6717 G Loss: 4.5256\n","Epoch [409/480] Batch [10/32] D Loss: 0.6769 G Loss: 4.8958\n","Epoch [409/480] Batch [20/32] D Loss: 0.6831 G Loss: 4.0778\n","Epoch [409/480] Batch [30/32] D Loss: 0.6732 G Loss: 4.1655\n","Epoch [410/480] Batch [0/32] D Loss: 0.6770 G Loss: 3.6353\n","Epoch [410/480] Batch [10/32] D Loss: 0.6912 G Loss: 4.2277\n","Epoch [410/480] Batch [20/32] D Loss: 0.6738 G Loss: 3.6258\n","Epoch [410/480] Batch [30/32] D Loss: 0.6794 G Loss: 4.1546\n","Epoch [411/480] Batch [0/32] D Loss: 0.6781 G Loss: 4.2989\n","Epoch [411/480] Batch [10/32] D Loss: 0.6947 G Loss: 3.9979\n","Epoch [411/480] Batch [20/32] D Loss: 0.6813 G Loss: 4.1955\n","Epoch [411/480] Batch [30/32] D Loss: 0.6900 G Loss: 4.2921\n","Epoch [412/480] Batch [0/32] D Loss: 0.6683 G Loss: 4.4738\n","Epoch [412/480] Batch [10/32] D Loss: 0.6812 G Loss: 4.0135\n","Epoch [412/480] Batch [20/32] D Loss: 0.6575 G Loss: 3.9741\n","Epoch [412/480] Batch [30/32] D Loss: 0.6784 G Loss: 5.1346\n","Epoch [413/480] Batch [0/32] D Loss: 0.6839 G Loss: 4.7256\n","Epoch [413/480] Batch [10/32] D Loss: 0.6605 G Loss: 4.6563\n","Epoch [413/480] Batch [20/32] D Loss: 0.6795 G Loss: 4.6164\n","Epoch [413/480] Batch [30/32] D Loss: 0.6845 G Loss: 4.3896\n","Epoch [414/480] Batch [0/32] D Loss: 0.6663 G Loss: 4.3133\n","Epoch [414/480] Batch [10/32] D Loss: 0.6648 G Loss: 5.1861\n","Epoch [414/480] Batch [20/32] D Loss: 0.6719 G Loss: 4.4608\n","Epoch [414/480] Batch [30/32] D Loss: 0.6784 G Loss: 3.8900\n","Epoch [415/480] Batch [0/32] D Loss: 0.6914 G Loss: 4.5728\n","Epoch [415/480] Batch [10/32] D Loss: 0.6698 G Loss: 3.7050\n","Epoch [415/480] Batch [20/32] D Loss: 0.6873 G Loss: 3.8845\n","Epoch [415/480] Batch [30/32] D Loss: 0.6763 G Loss: 4.3633\n","Epoch [416/480] Batch [0/32] D Loss: 0.6826 G Loss: 4.1419\n","Epoch [416/480] Batch [10/32] D Loss: 0.6761 G Loss: 4.1101\n","Epoch [416/480] Batch [20/32] D Loss: 0.6673 G Loss: 4.6404\n","Epoch [416/480] Batch [30/32] D Loss: 0.6925 G Loss: 4.0221\n","Epoch [417/480] Batch [0/32] D Loss: 0.6682 G Loss: 4.4041\n","Epoch [417/480] Batch [10/32] D Loss: 0.6871 G Loss: 4.8703\n","Epoch [417/480] Batch [20/32] D Loss: 0.6794 G Loss: 4.2027\n","Epoch [417/480] Batch [30/32] D Loss: 0.6858 G Loss: 3.8455\n","Epoch [418/480] Batch [0/32] D Loss: 0.6855 G Loss: 4.4906\n","Epoch [418/480] Batch [10/32] D Loss: 0.6830 G Loss: 4.4543\n","Epoch [418/480] Batch [20/32] D Loss: 0.6792 G Loss: 4.3322\n","Epoch [418/480] Batch [30/32] D Loss: 0.6745 G Loss: 4.0382\n","Epoch [419/480] Batch [0/32] D Loss: 0.6805 G Loss: 4.1072\n","Epoch [419/480] Batch [10/32] D Loss: 0.6901 G Loss: 4.2147\n","Epoch [419/480] Batch [20/32] D Loss: 0.6837 G Loss: 3.9560\n","Epoch [419/480] Batch [30/32] D Loss: 0.6797 G Loss: 4.5416\n","Epoch [420/480] Batch [0/32] D Loss: 0.6825 G Loss: 4.0601\n","Epoch [420/480] Batch [10/32] D Loss: 0.6837 G Loss: 4.4504\n","Epoch [420/480] Batch [20/32] D Loss: 0.6823 G Loss: 4.1756\n","Epoch [420/480] Batch [30/32] D Loss: 0.6765 G Loss: 4.4972\n","Epoch [421/480] Batch [0/32] D Loss: 0.6784 G Loss: 3.9717\n","Epoch [421/480] Batch [10/32] D Loss: 0.6696 G Loss: 3.9379\n","Epoch [421/480] Batch [20/32] D Loss: 0.7037 G Loss: 4.4695\n","Epoch [421/480] Batch [30/32] D Loss: 0.6689 G Loss: 4.5616\n","Epoch [422/480] Batch [0/32] D Loss: 0.6771 G Loss: 4.2432\n","Epoch [422/480] Batch [10/32] D Loss: 0.6627 G Loss: 4.2420\n","Epoch [422/480] Batch [20/32] D Loss: 0.6742 G Loss: 3.9785\n","Epoch [422/480] Batch [30/32] D Loss: 0.6754 G Loss: 3.9659\n","Epoch [423/480] Batch [0/32] D Loss: 0.6845 G Loss: 3.8027\n","Epoch [423/480] Batch [10/32] D Loss: 0.6665 G Loss: 4.0736\n","Epoch [423/480] Batch [20/32] D Loss: 0.6781 G Loss: 4.3015\n","Epoch [423/480] Batch [30/32] D Loss: 0.6781 G Loss: 4.3500\n","Epoch [424/480] Batch [0/32] D Loss: 0.6838 G Loss: 3.8465\n","Epoch [424/480] Batch [10/32] D Loss: 0.6727 G Loss: 4.1142\n","Epoch [424/480] Batch [20/32] D Loss: 0.6729 G Loss: 4.2700\n","Epoch [424/480] Batch [30/32] D Loss: 0.6643 G Loss: 4.2702\n","Epoch [425/480] Batch [0/32] D Loss: 0.6744 G Loss: 4.0050\n","Epoch [425/480] Batch [10/32] D Loss: 0.6709 G Loss: 4.3801\n","Epoch [425/480] Batch [20/32] D Loss: 0.6776 G Loss: 4.4610\n","Epoch [425/480] Batch [30/32] D Loss: 0.6698 G Loss: 4.5941\n","Epoch [426/480] Batch [0/32] D Loss: 0.6904 G Loss: 4.3752\n","Epoch [426/480] Batch [10/32] D Loss: 0.7037 G Loss: 5.2466\n","Epoch [426/480] Batch [20/32] D Loss: 0.6733 G Loss: 4.6195\n","Epoch [426/480] Batch [30/32] D Loss: 0.6785 G Loss: 4.0951\n","Epoch [427/480] Batch [0/32] D Loss: 0.6698 G Loss: 4.8383\n","Epoch [427/480] Batch [10/32] D Loss: 0.6747 G Loss: 4.3196\n","Epoch [427/480] Batch [20/32] D Loss: 0.6882 G Loss: 4.2998\n","Epoch [427/480] Batch [30/32] D Loss: 0.6747 G Loss: 4.1290\n","Epoch [428/480] Batch [0/32] D Loss: 0.6819 G Loss: 4.1664\n","Epoch [428/480] Batch [10/32] D Loss: 0.6725 G Loss: 3.9892\n","Epoch [428/480] Batch [20/32] D Loss: 0.6729 G Loss: 4.3321\n","Epoch [428/480] Batch [30/32] D Loss: 0.6815 G Loss: 4.6569\n","Epoch [429/480] Batch [0/32] D Loss: 0.6687 G Loss: 4.8040\n","Epoch [429/480] Batch [10/32] D Loss: 0.6815 G Loss: 4.1319\n","Epoch [429/480] Batch [20/32] D Loss: 0.6711 G Loss: 4.0956\n","Epoch [429/480] Batch [30/32] D Loss: 0.6799 G Loss: 4.6569\n","Epoch [430/480] Batch [0/32] D Loss: 0.6832 G Loss: 3.8026\n","Epoch [430/480] Batch [10/32] D Loss: 0.6752 G Loss: 4.3693\n","Epoch [430/480] Batch [20/32] D Loss: 0.6787 G Loss: 4.3090\n","Epoch [430/480] Batch [30/32] D Loss: 0.6858 G Loss: 4.0504\n","Epoch [431/480] Batch [0/32] D Loss: 0.6732 G Loss: 4.3027\n","Epoch [431/480] Batch [10/32] D Loss: 0.6867 G Loss: 4.2746\n","Epoch [431/480] Batch [20/32] D Loss: 0.6718 G Loss: 4.2385\n","Epoch [431/480] Batch [30/32] D Loss: 0.6852 G Loss: 4.3297\n","Epoch [432/480] Batch [0/32] D Loss: 0.6725 G Loss: 3.9787\n","Epoch [432/480] Batch [10/32] D Loss: 0.6702 G Loss: 4.1005\n","Epoch [432/480] Batch [20/32] D Loss: 0.6812 G Loss: 4.1312\n","Epoch [432/480] Batch [30/32] D Loss: 0.6789 G Loss: 3.8727\n","Epoch [433/480] Batch [0/32] D Loss: 0.6789 G Loss: 3.9459\n","Epoch [433/480] Batch [10/32] D Loss: 0.6659 G Loss: 4.5817\n","Epoch [433/480] Batch [20/32] D Loss: 0.6834 G Loss: 4.3214\n","Epoch [433/480] Batch [30/32] D Loss: 0.6808 G Loss: 3.6940\n","Epoch [434/480] Batch [0/32] D Loss: 0.6855 G Loss: 4.2912\n","Epoch [434/480] Batch [10/32] D Loss: 0.6796 G Loss: 4.6302\n","Epoch [434/480] Batch [20/32] D Loss: 0.6702 G Loss: 5.0767\n","Epoch [434/480] Batch [30/32] D Loss: 0.6909 G Loss: 3.9009\n","Epoch [435/480] Batch [0/32] D Loss: 0.6743 G Loss: 4.5960\n","Epoch [435/480] Batch [10/32] D Loss: 0.6806 G Loss: 4.1463\n","Epoch [435/480] Batch [20/32] D Loss: 0.6693 G Loss: 4.2086\n","Epoch [435/480] Batch [30/32] D Loss: 0.6697 G Loss: 4.8602\n","Epoch [436/480] Batch [0/32] D Loss: 0.6734 G Loss: 4.3147\n","Epoch [436/480] Batch [10/32] D Loss: 0.6641 G Loss: 4.3609\n","Epoch [436/480] Batch [20/32] D Loss: 0.6801 G Loss: 4.6837\n","Epoch [436/480] Batch [30/32] D Loss: 0.6828 G Loss: 4.3012\n","Epoch [437/480] Batch [0/32] D Loss: 0.6668 G Loss: 3.9859\n","Epoch [437/480] Batch [10/32] D Loss: 0.6930 G Loss: 3.7254\n","Epoch [437/480] Batch [20/32] D Loss: 0.6677 G Loss: 3.8211\n","Epoch [437/480] Batch [30/32] D Loss: 0.6751 G Loss: 3.9340\n","Epoch [438/480] Batch [0/32] D Loss: 0.6710 G Loss: 4.6643\n","Epoch [438/480] Batch [10/32] D Loss: 0.6964 G Loss: 4.4436\n","Epoch [438/480] Batch [20/32] D Loss: 0.6599 G Loss: 3.3755\n","Epoch [438/480] Batch [30/32] D Loss: 0.6742 G Loss: 4.3768\n","Epoch [439/480] Batch [0/32] D Loss: 0.6601 G Loss: 4.0189\n","Epoch [439/480] Batch [10/32] D Loss: 0.6791 G Loss: 3.6374\n","Epoch [439/480] Batch [20/32] D Loss: 0.6779 G Loss: 4.4665\n","Epoch [439/480] Batch [30/32] D Loss: 0.6785 G Loss: 4.1165\n","Epoch [440/480] Batch [0/32] D Loss: 0.6683 G Loss: 4.1588\n","Epoch [440/480] Batch [10/32] D Loss: 0.6684 G Loss: 4.5280\n","Epoch [440/480] Batch [20/32] D Loss: 0.6750 G Loss: 4.4398\n","Epoch [440/480] Batch [30/32] D Loss: 0.6756 G Loss: 4.7146\n","Epoch [441/480] Batch [0/32] D Loss: 0.6737 G Loss: 4.2919\n","Epoch [441/480] Batch [10/32] D Loss: 0.6837 G Loss: 4.8673\n","Epoch [441/480] Batch [20/32] D Loss: 0.6761 G Loss: 4.3773\n","Epoch [441/480] Batch [30/32] D Loss: 0.6848 G Loss: 4.0588\n","Epoch [442/480] Batch [0/32] D Loss: 0.6878 G Loss: 4.3097\n","Epoch [442/480] Batch [10/32] D Loss: 0.6839 G Loss: 4.4060\n","Epoch [442/480] Batch [20/32] D Loss: 0.6773 G Loss: 4.3985\n","Epoch [442/480] Batch [30/32] D Loss: 0.6652 G Loss: 4.4749\n","Epoch [443/480] Batch [0/32] D Loss: 0.6679 G Loss: 4.7341\n","Epoch [443/480] Batch [10/32] D Loss: 0.6785 G Loss: 4.2691\n","Epoch [443/480] Batch [20/32] D Loss: 0.6760 G Loss: 3.7309\n","Epoch [443/480] Batch [30/32] D Loss: 0.6869 G Loss: 3.8685\n","Epoch [444/480] Batch [0/32] D Loss: 0.6618 G Loss: 4.6003\n","Epoch [444/480] Batch [10/32] D Loss: 0.6844 G Loss: 4.0350\n","Epoch [444/480] Batch [20/32] D Loss: 0.6617 G Loss: 4.7489\n","Epoch [444/480] Batch [30/32] D Loss: 0.6757 G Loss: 3.7044\n","Epoch [445/480] Batch [0/32] D Loss: 0.6826 G Loss: 3.9576\n","Epoch [445/480] Batch [10/32] D Loss: 0.6816 G Loss: 4.4105\n","Epoch [445/480] Batch [20/32] D Loss: 0.6822 G Loss: 4.0104\n","Epoch [445/480] Batch [30/32] D Loss: 0.6908 G Loss: 3.9004\n","Epoch [446/480] Batch [0/32] D Loss: 0.6783 G Loss: 4.1842\n","Epoch [446/480] Batch [10/32] D Loss: 0.6825 G Loss: 4.5207\n","Epoch [446/480] Batch [20/32] D Loss: 0.6895 G Loss: 3.6031\n","Epoch [446/480] Batch [30/32] D Loss: 0.6700 G Loss: 4.8524\n","Epoch [447/480] Batch [0/32] D Loss: 0.6922 G Loss: 4.1276\n","Epoch [447/480] Batch [10/32] D Loss: 0.6807 G Loss: 4.1591\n","Epoch [447/480] Batch [20/32] D Loss: 0.6771 G Loss: 4.4352\n","Epoch [447/480] Batch [30/32] D Loss: 0.6664 G Loss: 4.6100\n","Epoch [448/480] Batch [0/32] D Loss: 0.6664 G Loss: 4.0023\n","Epoch [448/480] Batch [10/32] D Loss: 0.6829 G Loss: 4.8142\n","Epoch [448/480] Batch [20/32] D Loss: 0.6684 G Loss: 4.0271\n","Epoch [448/480] Batch [30/32] D Loss: 0.6903 G Loss: 4.2646\n","Epoch [449/480] Batch [0/32] D Loss: 0.6726 G Loss: 3.9169\n","Epoch [449/480] Batch [10/32] D Loss: 0.6856 G Loss: 4.4599\n","Epoch [449/480] Batch [20/32] D Loss: 0.6762 G Loss: 3.8553\n","Epoch [449/480] Batch [30/32] D Loss: 0.6875 G Loss: 3.9428\n","Epoch [450/480] Batch [0/32] D Loss: 0.6689 G Loss: 4.2196\n","Epoch [450/480] Batch [10/32] D Loss: 0.6598 G Loss: 4.7840\n","Epoch [450/480] Batch [20/32] D Loss: 0.6946 G Loss: 4.3969\n","Epoch [450/480] Batch [30/32] D Loss: 0.6711 G Loss: 4.0773\n","Epoch [451/480] Batch [0/32] D Loss: 0.6977 G Loss: 4.6735\n","Epoch [451/480] Batch [10/32] D Loss: 0.6805 G Loss: 4.1437\n","Epoch [451/480] Batch [20/32] D Loss: 0.6821 G Loss: 3.7093\n","Epoch [451/480] Batch [30/32] D Loss: 0.6834 G Loss: 3.9917\n","Epoch [452/480] Batch [0/32] D Loss: 0.6782 G Loss: 4.1406\n","Epoch [452/480] Batch [10/32] D Loss: 0.6788 G Loss: 3.7626\n","Epoch [452/480] Batch [20/32] D Loss: 0.6761 G Loss: 3.7641\n","Epoch [452/480] Batch [30/32] D Loss: 0.6787 G Loss: 4.0020\n","Epoch [453/480] Batch [0/32] D Loss: 0.6900 G Loss: 4.0264\n","Epoch [453/480] Batch [10/32] D Loss: 0.6880 G Loss: 3.5765\n","Epoch [453/480] Batch [20/32] D Loss: 0.6724 G Loss: 4.4723\n","Epoch [453/480] Batch [30/32] D Loss: 0.6758 G Loss: 3.7033\n","Epoch [454/480] Batch [0/32] D Loss: 0.6607 G Loss: 3.9008\n","Epoch [454/480] Batch [10/32] D Loss: 0.6882 G Loss: 3.7552\n","Epoch [454/480] Batch [20/32] D Loss: 0.6772 G Loss: 4.2356\n","Epoch [454/480] Batch [30/32] D Loss: 0.6935 G Loss: 5.1976\n","Epoch [455/480] Batch [0/32] D Loss: 0.6692 G Loss: 4.1213\n","Epoch [455/480] Batch [10/32] D Loss: 0.6833 G Loss: 4.1513\n","Epoch [455/480] Batch [20/32] D Loss: 0.6773 G Loss: 4.6897\n","Epoch [455/480] Batch [30/32] D Loss: 0.6856 G Loss: 3.6433\n","Epoch [456/480] Batch [0/32] D Loss: 0.7045 G Loss: 3.7546\n","Epoch [456/480] Batch [10/32] D Loss: 0.6730 G Loss: 4.5349\n","Epoch [456/480] Batch [20/32] D Loss: 0.6882 G Loss: 4.2450\n","Epoch [456/480] Batch [30/32] D Loss: 0.6683 G Loss: 3.7742\n","Epoch [457/480] Batch [0/32] D Loss: 0.6817 G Loss: 4.2841\n","Epoch [457/480] Batch [10/32] D Loss: 0.6760 G Loss: 4.4141\n","Epoch [457/480] Batch [20/32] D Loss: 0.6906 G Loss: 3.9884\n","Epoch [457/480] Batch [30/32] D Loss: 0.6736 G Loss: 4.7013\n","Epoch [458/480] Batch [0/32] D Loss: 0.6845 G Loss: 4.3536\n","Epoch [458/480] Batch [10/32] D Loss: 0.6689 G Loss: 4.9450\n","Epoch [458/480] Batch [20/32] D Loss: 0.6788 G Loss: 4.9017\n","Epoch [458/480] Batch [30/32] D Loss: 0.6717 G Loss: 4.9792\n","Epoch [459/480] Batch [0/32] D Loss: 0.6651 G Loss: 4.1862\n","Epoch [459/480] Batch [10/32] D Loss: 0.6872 G Loss: 4.0599\n","Epoch [459/480] Batch [20/32] D Loss: 0.6751 G Loss: 4.4471\n","Epoch [459/480] Batch [30/32] D Loss: 0.6609 G Loss: 4.3166\n","Epoch [460/480] Batch [0/32] D Loss: 0.6783 G Loss: 3.9976\n","Epoch [460/480] Batch [10/32] D Loss: 0.6797 G Loss: 3.9199\n","Epoch [460/480] Batch [20/32] D Loss: 0.6683 G Loss: 4.2565\n","Epoch [460/480] Batch [30/32] D Loss: 0.6751 G Loss: 4.1526\n","Epoch [461/480] Batch [0/32] D Loss: 0.6720 G Loss: 3.8658\n","Epoch [461/480] Batch [10/32] D Loss: 0.6803 G Loss: 4.9669\n","Epoch [461/480] Batch [20/32] D Loss: 0.6618 G Loss: 4.3448\n","Epoch [461/480] Batch [30/32] D Loss: 0.6763 G Loss: 4.0601\n","Epoch [462/480] Batch [0/32] D Loss: 0.6707 G Loss: 4.3458\n","Epoch [462/480] Batch [10/32] D Loss: 0.6598 G Loss: 4.3343\n","Epoch [462/480] Batch [20/32] D Loss: 0.6637 G Loss: 3.8619\n","Epoch [462/480] Batch [30/32] D Loss: 0.6815 G Loss: 3.9657\n","Epoch [463/480] Batch [0/32] D Loss: 0.6720 G Loss: 4.2913\n","Epoch [463/480] Batch [10/32] D Loss: 0.6778 G Loss: 3.6871\n","Epoch [463/480] Batch [20/32] D Loss: 0.6712 G Loss: 4.2145\n","Epoch [463/480] Batch [30/32] D Loss: 0.6796 G Loss: 3.7357\n","Epoch [464/480] Batch [0/32] D Loss: 0.6932 G Loss: 3.8704\n","Epoch [464/480] Batch [10/32] D Loss: 0.6681 G Loss: 3.9417\n","Epoch [464/480] Batch [20/32] D Loss: 0.6783 G Loss: 3.4966\n","Epoch [464/480] Batch [30/32] D Loss: 0.6645 G Loss: 4.5726\n","Epoch [465/480] Batch [0/32] D Loss: 0.6838 G Loss: 4.9883\n","Epoch [465/480] Batch [10/32] D Loss: 0.6709 G Loss: 4.3491\n","Epoch [465/480] Batch [20/32] D Loss: 0.6711 G Loss: 3.9000\n","Epoch [465/480] Batch [30/32] D Loss: 0.6761 G Loss: 3.6807\n","Epoch [466/480] Batch [0/32] D Loss: 0.6679 G Loss: 4.2542\n","Epoch [466/480] Batch [10/32] D Loss: 0.6713 G Loss: 4.1261\n","Epoch [466/480] Batch [20/32] D Loss: 0.6704 G Loss: 3.8306\n","Epoch [466/480] Batch [30/32] D Loss: 0.6782 G Loss: 3.8578\n","Epoch [467/480] Batch [0/32] D Loss: 0.6695 G Loss: 4.8075\n","Epoch [467/480] Batch [10/32] D Loss: 0.6764 G Loss: 4.5081\n","Epoch [467/480] Batch [20/32] D Loss: 0.6546 G Loss: 4.3197\n","Epoch [467/480] Batch [30/32] D Loss: 0.6923 G Loss: 4.3004\n","Epoch [468/480] Batch [0/32] D Loss: 0.6915 G Loss: 3.9271\n","Epoch [468/480] Batch [10/32] D Loss: 0.6771 G Loss: 3.7821\n","Epoch [468/480] Batch [20/32] D Loss: 0.6689 G Loss: 4.1034\n","Epoch [468/480] Batch [30/32] D Loss: 0.6758 G Loss: 4.2524\n","Epoch [469/480] Batch [0/32] D Loss: 0.6761 G Loss: 3.9343\n","Epoch [469/480] Batch [10/32] D Loss: 0.6627 G Loss: 4.1097\n","Epoch [469/480] Batch [20/32] D Loss: 0.6509 G Loss: 4.1353\n","Epoch [469/480] Batch [30/32] D Loss: 0.6963 G Loss: 3.7303\n","Epoch [470/480] Batch [0/32] D Loss: 0.6849 G Loss: 4.2975\n","Epoch [470/480] Batch [10/32] D Loss: 0.6690 G Loss: 4.0746\n","Epoch [470/480] Batch [20/32] D Loss: 0.6826 G Loss: 4.1853\n","Epoch [470/480] Batch [30/32] D Loss: 0.6695 G Loss: 4.3572\n","Epoch [471/480] Batch [0/32] D Loss: 0.6788 G Loss: 4.5057\n","Epoch [471/480] Batch [10/32] D Loss: 0.6626 G Loss: 4.1463\n","Epoch [471/480] Batch [20/32] D Loss: 0.6807 G Loss: 4.1107\n","Epoch [471/480] Batch [30/32] D Loss: 0.6764 G Loss: 4.4178\n","Epoch [472/480] Batch [0/32] D Loss: 0.6770 G Loss: 4.1550\n","Epoch [472/480] Batch [10/32] D Loss: 0.6690 G Loss: 4.4215\n","Epoch [472/480] Batch [20/32] D Loss: 0.6649 G Loss: 4.1491\n","Epoch [472/480] Batch [30/32] D Loss: 0.6944 G Loss: 3.9761\n","Epoch [473/480] Batch [0/32] D Loss: 0.6683 G Loss: 3.9844\n","Epoch [473/480] Batch [10/32] D Loss: 0.6963 G Loss: 4.6312\n","Epoch [473/480] Batch [20/32] D Loss: 0.6741 G Loss: 4.3984\n","Epoch [473/480] Batch [30/32] D Loss: 0.6630 G Loss: 4.4568\n","Epoch [474/480] Batch [0/32] D Loss: 0.6789 G Loss: 5.0403\n","Epoch [474/480] Batch [10/32] D Loss: 0.6674 G Loss: 4.4497\n","Epoch [474/480] Batch [20/32] D Loss: 0.6643 G Loss: 4.0168\n","Epoch [474/480] Batch [30/32] D Loss: 0.6717 G Loss: 4.0106\n","Epoch [475/480] Batch [0/32] D Loss: 0.6762 G Loss: 4.6848\n","Epoch [475/480] Batch [10/32] D Loss: 0.6631 G Loss: 4.6765\n","Epoch [475/480] Batch [20/32] D Loss: 0.6833 G Loss: 3.7826\n","Epoch [475/480] Batch [30/32] D Loss: 0.6722 G Loss: 3.8740\n","Epoch [476/480] Batch [0/32] D Loss: 0.6649 G Loss: 4.5511\n","Epoch [476/480] Batch [10/32] D Loss: 0.6735 G Loss: 4.2921\n","Epoch [476/480] Batch [20/32] D Loss: 0.6810 G Loss: 4.5003\n","Epoch [476/480] Batch [30/32] D Loss: 0.6695 G Loss: 4.6675\n","Epoch [477/480] Batch [0/32] D Loss: 0.6718 G Loss: 3.8550\n","Epoch [477/480] Batch [10/32] D Loss: 0.6611 G Loss: 4.3403\n","Epoch [477/480] Batch [20/32] D Loss: 0.6889 G Loss: 4.7367\n","Epoch [477/480] Batch [30/32] D Loss: 0.6718 G Loss: 3.8359\n","Epoch [478/480] Batch [0/32] D Loss: 0.6940 G Loss: 4.1044\n","Epoch [478/480] Batch [10/32] D Loss: 0.6733 G Loss: 4.0439\n","Epoch [478/480] Batch [20/32] D Loss: 0.6681 G Loss: 4.0600\n","Epoch [478/480] Batch [30/32] D Loss: 0.6638 G Loss: 4.5125\n","Epoch [479/480] Batch [0/32] D Loss: 0.6713 G Loss: 4.5047\n","Epoch [479/480] Batch [10/32] D Loss: 0.6887 G Loss: 3.9345\n","Epoch [479/480] Batch [20/32] D Loss: 0.6635 G Loss: 4.1942\n","Epoch [479/480] Batch [30/32] D Loss: 0.6778 G Loss: 4.1050\n","Epoch [480/480] Batch [0/32] D Loss: 0.6643 G Loss: 4.1617\n","Epoch [480/480] Batch [10/32] D Loss: 0.6795 G Loss: 4.5395\n","Epoch [480/480] Batch [20/32] D Loss: 0.6742 G Loss: 4.4316\n","Epoch [480/480] Batch [30/32] D Loss: 0.6733 G Loss: 3.8832\n"]}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    checkpoint_path = \"/content/drive/MyDrive/UPDATE/generator_epoch_480.pth\"\n","    train_gan(epochs=120,batch_size=32, start_epoch=481, checkpoint_path=checkpoint_path, lr=0.000005)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9ndLBJgVPsR","outputId":"81a24376-a8fa-4218-cde4-8b2f5f2c4f68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded checkpoint from /content/drive/MyDrive/UPDATE/generator_epoch_480.pth\n","Epoch [481/600] Batch [0/16] D Loss: 0.6933 G Loss: 4.3548\n","Epoch [481/600] Batch [10/16] D Loss: 0.6931 G Loss: 3.9231\n","Epoch [482/600] Batch [0/16] D Loss: 0.6931 G Loss: 4.4005\n","Epoch [482/600] Batch [10/16] D Loss: 0.6928 G Loss: 3.8293\n","Epoch [483/600] Batch [0/16] D Loss: 0.6929 G Loss: 4.6152\n","Epoch [483/600] Batch [10/16] D Loss: 0.6927 G Loss: 3.6724\n","Epoch [484/600] Batch [0/16] D Loss: 0.6927 G Loss: 3.9255\n","Epoch [484/600] Batch [10/16] D Loss: 0.6923 G Loss: 4.2722\n","Epoch [485/600] Batch [0/16] D Loss: 0.6926 G Loss: 3.7828\n","Epoch [485/600] Batch [10/16] D Loss: 0.6924 G Loss: 4.2178\n","Epoch [486/600] Batch [0/16] D Loss: 0.6922 G Loss: 4.1196\n","Epoch [486/600] Batch [10/16] D Loss: 0.6920 G Loss: 3.9088\n","Epoch [487/600] Batch [0/16] D Loss: 0.6917 G Loss: 4.1003\n","Epoch [487/600] Batch [10/16] D Loss: 0.6918 G Loss: 3.9935\n","Epoch [488/600] Batch [0/16] D Loss: 0.6921 G Loss: 3.9413\n","Epoch [488/600] Batch [10/16] D Loss: 0.6913 G Loss: 4.0532\n","Epoch [489/600] Batch [0/16] D Loss: 0.6905 G Loss: 4.3156\n","Epoch [489/600] Batch [10/16] D Loss: 0.6907 G Loss: 4.2202\n","Epoch [490/600] Batch [0/16] D Loss: 0.6910 G Loss: 4.1525\n","Epoch [490/600] Batch [10/16] D Loss: 0.6916 G Loss: 3.9428\n","Epoch [491/600] Batch [0/16] D Loss: 0.6906 G Loss: 4.2274\n","Epoch [491/600] Batch [10/16] D Loss: 0.6894 G Loss: 3.9499\n","Epoch [492/600] Batch [0/16] D Loss: 0.6912 G Loss: 4.1946\n","Epoch [492/600] Batch [10/16] D Loss: 0.6911 G Loss: 4.5303\n","Epoch [493/600] Batch [0/16] D Loss: 0.6896 G Loss: 3.9588\n","Epoch [493/600] Batch [10/16] D Loss: 0.6911 G Loss: 3.9416\n","Epoch [494/600] Batch [0/16] D Loss: 0.6881 G Loss: 4.3719\n","Epoch [494/600] Batch [10/16] D Loss: 0.6896 G Loss: 3.9547\n","Epoch [495/600] Batch [0/16] D Loss: 0.6906 G Loss: 4.0600\n","Epoch [495/600] Batch [10/16] D Loss: 0.6889 G Loss: 4.3525\n","Epoch [496/600] Batch [0/16] D Loss: 0.6903 G Loss: 3.9048\n","Epoch [496/600] Batch [10/16] D Loss: 0.6876 G Loss: 4.1627\n","Epoch [497/600] Batch [0/16] D Loss: 0.6908 G Loss: 4.4441\n","Epoch [497/600] Batch [10/16] D Loss: 0.6877 G Loss: 3.8872\n","Epoch [498/600] Batch [0/16] D Loss: 0.6883 G Loss: 4.1563\n","Epoch [498/600] Batch [10/16] D Loss: 0.6873 G Loss: 4.0210\n","Epoch [499/600] Batch [0/16] D Loss: 0.6858 G Loss: 4.4157\n","Epoch [499/600] Batch [10/16] D Loss: 0.6857 G Loss: 3.9883\n","Epoch [500/600] Batch [0/16] D Loss: 0.6831 G Loss: 4.5027\n","Epoch [500/600] Batch [10/16] D Loss: 0.6905 G Loss: 3.9282\n","Epoch [501/600] Batch [0/16] D Loss: 0.6840 G Loss: 4.2538\n","Epoch [501/600] Batch [10/16] D Loss: 0.6835 G Loss: 4.1344\n","Epoch [502/600] Batch [0/16] D Loss: 0.6819 G Loss: 4.3171\n","Epoch [502/600] Batch [10/16] D Loss: 0.6940 G Loss: 4.1031\n","Epoch [503/600] Batch [0/16] D Loss: 0.6892 G Loss: 4.0291\n","Epoch [503/600] Batch [10/16] D Loss: 0.6868 G Loss: 4.2594\n","Epoch [504/600] Batch [0/16] D Loss: 0.6820 G Loss: 4.1618\n","Epoch [504/600] Batch [10/16] D Loss: 0.6841 G Loss: 4.2212\n","Epoch [505/600] Batch [0/16] D Loss: 0.6808 G Loss: 4.5166\n","Epoch [505/600] Batch [10/16] D Loss: 0.6848 G Loss: 4.2407\n","Epoch [506/600] Batch [0/16] D Loss: 0.6832 G Loss: 4.4405\n","Epoch [506/600] Batch [10/16] D Loss: 0.6818 G Loss: 4.4325\n","Epoch [507/600] Batch [0/16] D Loss: 0.6871 G Loss: 4.3599\n","Epoch [507/600] Batch [10/16] D Loss: 0.6835 G Loss: 4.1838\n","Epoch [508/600] Batch [0/16] D Loss: 0.6783 G Loss: 4.0503\n","Epoch [508/600] Batch [10/16] D Loss: 0.6891 G Loss: 4.2562\n","Epoch [509/600] Batch [0/16] D Loss: 0.6810 G Loss: 4.2822\n","Epoch [509/600] Batch [10/16] D Loss: 0.6936 G Loss: 4.2901\n","Epoch [510/600] Batch [0/16] D Loss: 0.6887 G Loss: 4.1287\n","Epoch [510/600] Batch [10/16] D Loss: 0.6799 G Loss: 3.9095\n","Epoch [511/600] Batch [0/16] D Loss: 0.6905 G Loss: 4.2839\n","Epoch [511/600] Batch [10/16] D Loss: 0.6814 G Loss: 4.6280\n","Epoch [512/600] Batch [0/16] D Loss: 0.6889 G Loss: 4.0403\n","Epoch [512/600] Batch [10/16] D Loss: 0.6822 G Loss: 3.8420\n","Epoch [513/600] Batch [0/16] D Loss: 0.6856 G Loss: 4.1293\n","Epoch [513/600] Batch [10/16] D Loss: 0.6808 G Loss: 4.1873\n","Epoch [514/600] Batch [0/16] D Loss: 0.6848 G Loss: 4.0617\n","Epoch [514/600] Batch [10/16] D Loss: 0.6842 G Loss: 3.9560\n","Epoch [515/600] Batch [0/16] D Loss: 0.6753 G Loss: 4.2046\n","Epoch [515/600] Batch [10/16] D Loss: 0.6895 G Loss: 4.1985\n","Epoch [516/600] Batch [0/16] D Loss: 0.6778 G Loss: 4.1314\n","Epoch [516/600] Batch [10/16] D Loss: 0.6868 G Loss: 3.8681\n","Epoch [517/600] Batch [0/16] D Loss: 0.6809 G Loss: 3.9962\n","Epoch [517/600] Batch [10/16] D Loss: 0.6805 G Loss: 3.7790\n","Epoch [518/600] Batch [0/16] D Loss: 0.6883 G Loss: 4.1480\n","Epoch [518/600] Batch [10/16] D Loss: 0.6899 G Loss: 4.4389\n","Epoch [519/600] Batch [0/16] D Loss: 0.6891 G Loss: 4.0511\n","Epoch [519/600] Batch [10/16] D Loss: 0.6834 G Loss: 4.4950\n","Epoch [520/600] Batch [0/16] D Loss: 0.6826 G Loss: 4.0764\n","Epoch [520/600] Batch [10/16] D Loss: 0.6802 G Loss: 4.0794\n","Epoch [521/600] Batch [0/16] D Loss: 0.6850 G Loss: 4.2593\n","Epoch [521/600] Batch [10/16] D Loss: 0.6865 G Loss: 4.5636\n","Epoch [522/600] Batch [0/16] D Loss: 0.6856 G Loss: 3.9287\n","Epoch [522/600] Batch [10/16] D Loss: 0.6921 G Loss: 3.9670\n","Epoch [523/600] Batch [0/16] D Loss: 0.6851 G Loss: 4.1421\n","Epoch [523/600] Batch [10/16] D Loss: 0.6874 G Loss: 4.5857\n","Epoch [524/600] Batch [0/16] D Loss: 0.6786 G Loss: 4.0265\n","Epoch [524/600] Batch [10/16] D Loss: 0.6873 G Loss: 3.9212\n","Epoch [525/600] Batch [0/16] D Loss: 0.6807 G Loss: 4.6421\n","Epoch [525/600] Batch [10/16] D Loss: 0.6839 G Loss: 4.1465\n","Epoch [526/600] Batch [0/16] D Loss: 0.6859 G Loss: 3.9066\n","Epoch [526/600] Batch [10/16] D Loss: 0.6880 G Loss: 4.0747\n","Epoch [527/600] Batch [0/16] D Loss: 0.6746 G Loss: 4.2398\n","Epoch [527/600] Batch [10/16] D Loss: 0.6835 G Loss: 3.9353\n","Epoch [528/600] Batch [0/16] D Loss: 0.6893 G Loss: 4.3757\n","Epoch [528/600] Batch [10/16] D Loss: 0.6907 G Loss: 4.1780\n","Epoch [529/600] Batch [0/16] D Loss: 0.6796 G Loss: 4.0711\n","Epoch [529/600] Batch [10/16] D Loss: 0.6899 G Loss: 4.4427\n","Epoch [530/600] Batch [0/16] D Loss: 0.6813 G Loss: 3.8399\n","Epoch [530/600] Batch [10/16] D Loss: 0.6769 G Loss: 4.3430\n","Epoch [531/600] Batch [0/16] D Loss: 0.6817 G Loss: 4.7915\n","Epoch [531/600] Batch [10/16] D Loss: 0.6859 G Loss: 3.9344\n","Epoch [532/600] Batch [0/16] D Loss: 0.6762 G Loss: 3.9822\n","Epoch [532/600] Batch [10/16] D Loss: 0.6721 G Loss: 4.3849\n","Epoch [533/600] Batch [0/16] D Loss: 0.6831 G Loss: 4.0091\n","Epoch [533/600] Batch [10/16] D Loss: 0.6824 G Loss: 4.1590\n","Epoch [534/600] Batch [0/16] D Loss: 0.6734 G Loss: 4.4290\n","Epoch [534/600] Batch [10/16] D Loss: 0.6848 G Loss: 4.1127\n","Epoch [535/600] Batch [0/16] D Loss: 0.6809 G Loss: 4.0099\n","Epoch [535/600] Batch [10/16] D Loss: 0.6829 G Loss: 4.2450\n","Epoch [536/600] Batch [0/16] D Loss: 0.6773 G Loss: 4.1002\n","Epoch [536/600] Batch [10/16] D Loss: 0.6833 G Loss: 3.8684\n","Epoch [537/600] Batch [0/16] D Loss: 0.6744 G Loss: 4.4269\n","Epoch [537/600] Batch [10/16] D Loss: 0.6836 G Loss: 4.2896\n","Epoch [538/600] Batch [0/16] D Loss: 0.6891 G Loss: 4.0616\n","Epoch [538/600] Batch [10/16] D Loss: 0.6823 G Loss: 4.2845\n","Epoch [539/600] Batch [0/16] D Loss: 0.6851 G Loss: 4.8719\n","Epoch [539/600] Batch [10/16] D Loss: 0.6772 G Loss: 4.1658\n","Epoch [540/600] Batch [0/16] D Loss: 0.6866 G Loss: 4.5606\n","Epoch [540/600] Batch [10/16] D Loss: 0.6845 G Loss: 4.0520\n","Epoch [541/600] Batch [0/16] D Loss: 0.6859 G Loss: 3.8827\n","Epoch [541/600] Batch [10/16] D Loss: 0.6820 G Loss: 3.8292\n","Epoch [542/600] Batch [0/16] D Loss: 0.6856 G Loss: 3.9291\n","Epoch [542/600] Batch [10/16] D Loss: 0.6811 G Loss: 4.2743\n","Epoch [543/600] Batch [0/16] D Loss: 0.6741 G Loss: 4.1859\n","Epoch [543/600] Batch [10/16] D Loss: 0.6848 G Loss: 4.0923\n","Epoch [544/600] Batch [0/16] D Loss: 0.6891 G Loss: 3.7681\n","Epoch [544/600] Batch [10/16] D Loss: 0.6834 G Loss: 3.9609\n","Epoch [545/600] Batch [0/16] D Loss: 0.6813 G Loss: 4.5112\n","Epoch [545/600] Batch [10/16] D Loss: 0.6923 G Loss: 3.8560\n","Epoch [546/600] Batch [0/16] D Loss: 0.6742 G Loss: 4.1790\n","Epoch [546/600] Batch [10/16] D Loss: 0.6854 G Loss: 4.0893\n","Epoch [547/600] Batch [0/16] D Loss: 0.6890 G Loss: 3.9184\n","Epoch [547/600] Batch [10/16] D Loss: 0.6796 G Loss: 4.5742\n","Epoch [548/600] Batch [0/16] D Loss: 0.6743 G Loss: 4.1302\n","Epoch [548/600] Batch [10/16] D Loss: 0.6943 G Loss: 4.8274\n","Epoch [549/600] Batch [0/16] D Loss: 0.6839 G Loss: 3.9553\n","Epoch [549/600] Batch [10/16] D Loss: 0.6869 G Loss: 4.3361\n","Epoch [550/600] Batch [0/16] D Loss: 0.6755 G Loss: 4.1515\n","Epoch [550/600] Batch [10/16] D Loss: 0.6849 G Loss: 4.1616\n","Epoch [551/600] Batch [0/16] D Loss: 0.6822 G Loss: 3.9287\n","Epoch [551/600] Batch [10/16] D Loss: 0.6851 G Loss: 4.0204\n","Epoch [552/600] Batch [0/16] D Loss: 0.6762 G Loss: 3.9043\n","Epoch [552/600] Batch [10/16] D Loss: 0.6862 G Loss: 4.0352\n","Epoch [553/600] Batch [0/16] D Loss: 0.6744 G Loss: 4.0207\n","Epoch [553/600] Batch [10/16] D Loss: 0.6690 G Loss: 4.5217\n","Epoch [554/600] Batch [0/16] D Loss: 0.6884 G Loss: 4.2149\n","Epoch [554/600] Batch [10/16] D Loss: 0.6884 G Loss: 4.0730\n","Epoch [555/600] Batch [0/16] D Loss: 0.6829 G Loss: 4.1988\n","Epoch [555/600] Batch [10/16] D Loss: 0.6834 G Loss: 3.9392\n","Epoch [556/600] Batch [0/16] D Loss: 0.6864 G Loss: 4.2490\n","Epoch [556/600] Batch [10/16] D Loss: 0.6809 G Loss: 3.8971\n","Epoch [557/600] Batch [0/16] D Loss: 0.6719 G Loss: 4.3934\n","Epoch [557/600] Batch [10/16] D Loss: 0.6828 G Loss: 3.7667\n","Epoch [558/600] Batch [0/16] D Loss: 0.6763 G Loss: 4.3702\n","Epoch [558/600] Batch [10/16] D Loss: 0.6771 G Loss: 3.7662\n","Epoch [559/600] Batch [0/16] D Loss: 0.6802 G Loss: 3.9241\n","Epoch [559/600] Batch [10/16] D Loss: 0.6844 G Loss: 4.2707\n","Epoch [560/600] Batch [0/16] D Loss: 0.6863 G Loss: 4.1233\n","Epoch [560/600] Batch [10/16] D Loss: 0.6848 G Loss: 4.1983\n","Epoch [561/600] Batch [0/16] D Loss: 0.6863 G Loss: 4.1839\n","Epoch [561/600] Batch [10/16] D Loss: 0.6785 G Loss: 3.9705\n","Epoch [562/600] Batch [0/16] D Loss: 0.6789 G Loss: 4.1239\n"]}]}]}